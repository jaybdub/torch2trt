{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"torch2trt torch2trt is a PyTorch to TensorRT converter which utilizes the TensorRT Python API. The converter is Easy to use - Convert modules with a single function call torch2trt Easy to extend - Write your own layer converter in Python and register it with @tensorrt_converter If you find an issue, please let us know !","title":"Home"},{"location":"index.html#torch2trt","text":"torch2trt is a PyTorch to TensorRT converter which utilizes the TensorRT Python API. The converter is Easy to use - Convert modules with a single function call torch2trt Easy to extend - Write your own layer converter in Python and register it with @tensorrt_converter If you find an issue, please let us know !","title":"torch2trt"},{"location":"CHANGELOG.html","text":"Changes [Master] [0.2.0] - 03/02/2021 Added Added converter for torch.Tensor.expand Added support for custom converters for methods defined outside of torch module Added names for TensorRT layers Added GroupNorm plugin which internally uses PyTorch aten::group_norm Replaced Tensor.ndim references with len(tensor.shape) to support older pytorch versions Added reduced precision documentation page Added converters for floordiv , mod , ne , and torch.tensor operations Extended relu converter to support Tensor.relu operation Extended sigmoid converter to support Tensor.sigmoid operation","title":"Changes"},{"location":"CHANGELOG.html#changes","text":"","title":"Changes"},{"location":"CHANGELOG.html#master","text":"","title":"[Master]"},{"location":"CHANGELOG.html#020-03022021","text":"","title":"[0.2.0] - 03/02/2021"},{"location":"CHANGELOG.html#added","text":"Added converter for torch.Tensor.expand Added support for custom converters for methods defined outside of torch module Added names for TensorRT layers Added GroupNorm plugin which internally uses PyTorch aten::group_norm Replaced Tensor.ndim references with len(tensor.shape) to support older pytorch versions Added reduced precision documentation page Added converters for floordiv , mod , ne , and torch.tensor operations Extended relu converter to support Tensor.relu operation Extended sigmoid converter to support Tensor.sigmoid operation","title":"Added"},{"location":"CONTRIBUTING.html","text":"Contributing Forms of contribution Submit an Issue torch2trt is use case driven. We originally created it to solve use cases related to NVIDIA Jetson, but the layer support has grown largely since it's release and we've found that it has helped many other developers as well. The growth of torch2trt has been largely driven by issues submitted on GitHub . We learn a lot from the reported issues. Submitting an issue it is one of the best ways to begin contributing to torch2trt. The reported issues typically are one of the following, A bug or unexpected result A model with unsupported layers If you report an issue, we typically find the following information helpful PyTorch version TensorRT version Platform (ie: Jetson Nano) The PyTorch Module you're attempting to convert The steps taken to convert the PyTorch module If you're not sure how to provide any of these pieces of information, don't worry. Just open the issue and we're happy to discuss and help work out the details. Ask a Question Another great way to contribute is to ask a question on GitHub . There are often other developers who share your question, and they may find the discussion helpful. This also helps us gauge feature interest and identify gaps in documentation. Submit a Pull Request torch2trt is use case driven and has limited maintainence, for this reason we value community contributions greatly. Another great way to contribute is by submitting a pull request. Pull requests which are most likely to be accepted are A new converter A test case A bug fix If you add a new converter, it is best to include a few test cases that cross validate the converter against the original PyTorch. We provide a utility function to do this, as described in the Custom Converter usage guide. Ideally pull requests solve one thing at a time. This makes it easy to evaluate the impact that the changes have on the project step-by-step. The more confident we are that the changes will not adversely impact the experience of other developers, the more likely we are to accept them. Running module test cases Before any change is accepted, we run the test cases on at least one platform. This performs a large number of cross validation checks against PyTorch. To do this python3 -m torch2trt.test --name = converters --tolerance = 1e-2 This will not hard-fail, but will highlight any build errors or max error checks. It is helpful if you include the status of this command in any pull-request, as well as system information like PyTorch version TensorRT version Platform (ie: Jetson Nano) Testing documentation If you have a change that modifies the documentation, it is relatively straightforward to test. We use mkdocs-material for documentation, which parses markdown files in the docs folder. To view the docs, simply call ./scripts/test_docs.sh And then navigate to https://<ip_address>:8000 . Please note, this will not include dynamically generated documentation pages like the converters page. These contain cross reference links to the GitHub source code. If you want to test these you can call ./scripts/build_docs.sh <github url> <tag> Pointing to the public reflection of your local repository. For example, if we're working off the upstream master branch, we would call ./scripts/build_docs.sh https://github.com/NVIDIA-AI-IOT/torch2trt master If your changes are pushed to your fork, you would do ./scripts/build_docs.sh https://github.com/<user>/torch2trt my_branch","title":"Contributing"},{"location":"CONTRIBUTING.html#contributing","text":"","title":"Contributing"},{"location":"CONTRIBUTING.html#forms-of-contribution","text":"","title":"Forms of contribution"},{"location":"CONTRIBUTING.html#submit-an-issue","text":"torch2trt is use case driven. We originally created it to solve use cases related to NVIDIA Jetson, but the layer support has grown largely since it's release and we've found that it has helped many other developers as well. The growth of torch2trt has been largely driven by issues submitted on GitHub . We learn a lot from the reported issues. Submitting an issue it is one of the best ways to begin contributing to torch2trt. The reported issues typically are one of the following, A bug or unexpected result A model with unsupported layers If you report an issue, we typically find the following information helpful PyTorch version TensorRT version Platform (ie: Jetson Nano) The PyTorch Module you're attempting to convert The steps taken to convert the PyTorch module If you're not sure how to provide any of these pieces of information, don't worry. Just open the issue and we're happy to discuss and help work out the details.","title":"Submit an Issue"},{"location":"CONTRIBUTING.html#ask-a-question","text":"Another great way to contribute is to ask a question on GitHub . There are often other developers who share your question, and they may find the discussion helpful. This also helps us gauge feature interest and identify gaps in documentation.","title":"Ask a Question"},{"location":"CONTRIBUTING.html#submit-a-pull-request","text":"torch2trt is use case driven and has limited maintainence, for this reason we value community contributions greatly. Another great way to contribute is by submitting a pull request. Pull requests which are most likely to be accepted are A new converter A test case A bug fix If you add a new converter, it is best to include a few test cases that cross validate the converter against the original PyTorch. We provide a utility function to do this, as described in the Custom Converter usage guide. Ideally pull requests solve one thing at a time. This makes it easy to evaluate the impact that the changes have on the project step-by-step. The more confident we are that the changes will not adversely impact the experience of other developers, the more likely we are to accept them.","title":"Submit a Pull Request"},{"location":"CONTRIBUTING.html#running-module-test-cases","text":"Before any change is accepted, we run the test cases on at least one platform. This performs a large number of cross validation checks against PyTorch. To do this python3 -m torch2trt.test --name = converters --tolerance = 1e-2 This will not hard-fail, but will highlight any build errors or max error checks. It is helpful if you include the status of this command in any pull-request, as well as system information like PyTorch version TensorRT version Platform (ie: Jetson Nano)","title":"Running module test cases"},{"location":"CONTRIBUTING.html#testing-documentation","text":"If you have a change that modifies the documentation, it is relatively straightforward to test. We use mkdocs-material for documentation, which parses markdown files in the docs folder. To view the docs, simply call ./scripts/test_docs.sh And then navigate to https://<ip_address>:8000 . Please note, this will not include dynamically generated documentation pages like the converters page. These contain cross reference links to the GitHub source code. If you want to test these you can call ./scripts/build_docs.sh <github url> <tag> Pointing to the public reflection of your local repository. For example, if we're working off the upstream master branch, we would call ./scripts/build_docs.sh https://github.com/NVIDIA-AI-IOT/torch2trt master If your changes are pushed to your fork, you would do ./scripts/build_docs.sh https://github.com/<user>/torch2trt my_branch","title":"Testing documentation"},{"location":"converters.html","text":"Converters This table contains a list of supported PyTorch methods and their associated converters. If your model is not converting, a good start in debugging would be to see if it contains a method not listed in this table. You may also find these a useful reference when writing your own converters. Method Converter torch.abs convert_abs torch.abs_ convert_abs torch.acos convert_acos torch.acos_ convert_acos torch.add convert_add torch.asin convert_asin torch.asin_ convert_asin torch.atan convert_atan torch.atan_ convert_atan torch.cat convert_cat torch.ceil convert_ceil torch.ceil_ convert_ceil torch.chunk convert_chunk torch.clamp convert_clamp torch.clamp_max convert_clamp_max torch.clamp_min convert_clamp_min torch.cos convert_cos torch.cos_ convert_cos torch.cosh convert_cosh torch.cosh_ convert_cosh torch.div convert_div torch.eq convert_gt torch.exp convert_exp torch.exp_ convert_exp torch.flatten convert_view torch.floor convert_floor torch.floor_ convert_floor torch.floor_divide convert_floordiv torch.fmod convert_mod torch.gt convert_gt torch.instance_norm convert_instance_norm torch.log convert_log torch.log_ convert_log torch.lt convert_gt torch.max convert_max torch.mean convert_mean torch.min convert_min torch.mul convert_mul torch.narrow convert_narrow torch.ne convert_ne torch.neg convert_neg torch.neg_ convert_neg torch.pow convert_pow torch.prod convert_prod torch.reciprocal convert_reciprocal torch.reciprocal_ convert_reciprocal torch.relu convert_functional_relu torch.relu_ convert_functional_relu torch.selu convert_selu torch.selu_ convert_selu torch.sigmoid convert_sigmoid torch.sin convert_sin torch.sin_ convert_sin torch.sinh convert_sinh torch.sinh_ convert_sinh torch.split convert_split torch.sqrt convert_sqrt torch.sqrt_ convert_sqrt torch.squeeze convert_view torch.stack convert_cat_trt7 torch.sub convert_sub torch.sum convert_sum torch.tan convert_cos torch.tan_ convert_cos torch.tanh convert_tanh torch.tensor convert_mod torch.transpose convert_transpose_trt7 torch.unsqueeze convert_view torch.Tensor.__add__ convert_add torch.Tensor.__div__ convert_div torch.Tensor.__eq__ convert_gt torch.Tensor.__floordiv__ convert_floordiv torch.Tensor.__gt__ convert_gt torch.Tensor.__iadd__ convert_add torch.Tensor.__idiv__ convert_div torch.Tensor.__ifloordiv__ convert_floordiv torch.Tensor.__imul__ convert_mul torch.Tensor.__ipow__ convert_pow torch.Tensor.__isub__ convert_sub torch.Tensor.__itruediv__ convert_div torch.Tensor.__lt__ convert_gt torch.Tensor.__mod__ convert_mod torch.Tensor.__mul__ convert_mul torch.Tensor.__ne__ convert_ne torch.Tensor.__neg__ convert_neg torch.Tensor.__pow__ convert_pow torch.Tensor.__radd__ convert_add torch.Tensor.__rdiv__ convert_rdiv torch.Tensor.__rmul__ convert_mul torch.Tensor.__rpow__ convert_pow torch.Tensor.__rsub__ convert_sub torch.Tensor.__rtruediv__ convert_rdiv torch.Tensor.__sub__ convert_sub torch.Tensor.__truediv__ convert_div torch.Tensor.abs convert_abs torch.Tensor.abs_ convert_abs torch.Tensor.acos convert_acos torch.Tensor.acos_ convert_acos torch.Tensor.asin convert_asin torch.Tensor.asin_ convert_asin torch.Tensor.atan convert_atan torch.Tensor.atan_ convert_atan torch.Tensor.ceil convert_ceil torch.Tensor.ceil_ convert_ceil torch.Tensor.chunk convert_chunk torch.Tensor.clamp convert_clamp torch.Tensor.clamp_max convert_clamp_max torch.Tensor.clamp_min convert_clamp_min torch.Tensor.contiguous convert_functional_identity torch.Tensor.cos convert_cos torch.Tensor.cos_ convert_cos torch.Tensor.cosh convert_cosh torch.Tensor.cosh_ convert_cosh torch.Tensor.exp convert_exp torch.Tensor.exp_ convert_exp torch.Tensor.expand convert_expand torch.Tensor.floor convert_floor torch.Tensor.floor_ convert_floor torch.Tensor.log convert_log torch.Tensor.log_ convert_log torch.Tensor.max convert_max torch.Tensor.mean convert_mean torch.Tensor.min convert_min torch.Tensor.narrow convert_narrow torch.Tensor.neg convert_neg torch.Tensor.neg_ convert_neg torch.Tensor.permute convert_permute torch.Tensor.prod convert_prod torch.Tensor.reciprocal convert_reciprocal torch.Tensor.reciprocal_ convert_reciprocal torch.Tensor.relu convert_functional_relu torch.Tensor.reshape convert_view torch.Tensor.sigmoid convert_sigmoid torch.Tensor.sin convert_sin torch.Tensor.sin_ convert_sin torch.Tensor.sinh convert_sinh torch.Tensor.sinh_ convert_sinh torch.Tensor.split convert_split torch.Tensor.sqrt convert_sqrt torch.Tensor.sqrt_ convert_sqrt torch.Tensor.squeeze convert_view torch.Tensor.sum convert_sum torch.Tensor.tan convert_cos torch.Tensor.tan_ convert_cos torch.Tensor.unsqueeze convert_view torch.Tensor.view convert_view torch.nn.functional.adaptive_avg_pool2d convert_adaptive_avg_pool2d torch.nn.functional.adaptive_max_pool2d convert_adaptive_max_pool2d torch.nn.functional.avg_pool2d convert_avg_pool_trt7 torch.nn.functional.avg_pool3d convert_avg_pool_trt7 torch.nn.functional.batch_norm convert_batch_norm_trt7 torch.nn.functional.dropout convert_functional_identity torch.nn.functional.dropout2d convert_functional_identity torch.nn.functional.dropout3d convert_functional_identity torch.nn.functional.elu convert_elu torch.nn.functional.elu_ convert_elu torch.nn.functional.instance_norm convert_instance_norm torch.nn.functional.interpolate convert_interpolate_trt7 torch.nn.functional.leaky_relu convert_leaky_relu torch.nn.functional.leaky_relu_ convert_leaky_relu torch.nn.functional.max_pool2d convert_max_pool2d torch.nn.functional.normalize convert_normalize torch.nn.functional.pad convert_pad torch.nn.functional.prelu convert_prelu torch.nn.functional.relu convert_functional_relu torch.nn.functional.relu6 convert_functional_relu6 torch.nn.functional.relu_ convert_functional_relu torch.nn.functional.selu convert_selu torch.nn.functional.selu_ convert_selu torch.nn.functional.sigmoid convert_sigmoid torch.nn.functional.softmax convert_softmax torch.nn.functional.softplus convert_softplus torch.nn.functional.softsign convert_softsign torch.nn.functional.tanh convert_tanh torch.nn.functional.upsample convert_interpolate_trt7 torch.nn.AdaptiveAvgPool2d.forward convert_AdaptiveAvgPool2d torch.nn.BatchNorm1d.forward convert_BatchNorm2d torch.nn.Conv3d.forward convert_Conv_trt7 torch.nn.Conv2d.forward convert_Conv_trt7 torch.nn.Conv1d.forward convert_Conv1d torch.nn.ConvTranspose3d.forward convert_ConvTranspose2d_trt7 torch.nn.ConvTranspose2d.forward convert_ConvTranspose2d_trt7 torch.nn.Linear.forward convert_Linear torch.nn.LogSoftmax.forward convert_LogSoftmax torch.Tensor.__getitem__ convert_tensor_getitem torch.nn.Dropout3d.forward convert_identity torch.nn.Dropout2d.forward convert_identity torch.nn.Dropout.forward convert_identity torch.nn.ReLU.forward convert_relu torch.nn.ReLU6.forward convert_relu6","title":"Converters"},{"location":"converters.html#converters","text":"This table contains a list of supported PyTorch methods and their associated converters. If your model is not converting, a good start in debugging would be to see if it contains a method not listed in this table. You may also find these a useful reference when writing your own converters. Method Converter torch.abs convert_abs torch.abs_ convert_abs torch.acos convert_acos torch.acos_ convert_acos torch.add convert_add torch.asin convert_asin torch.asin_ convert_asin torch.atan convert_atan torch.atan_ convert_atan torch.cat convert_cat torch.ceil convert_ceil torch.ceil_ convert_ceil torch.chunk convert_chunk torch.clamp convert_clamp torch.clamp_max convert_clamp_max torch.clamp_min convert_clamp_min torch.cos convert_cos torch.cos_ convert_cos torch.cosh convert_cosh torch.cosh_ convert_cosh torch.div convert_div torch.eq convert_gt torch.exp convert_exp torch.exp_ convert_exp torch.flatten convert_view torch.floor convert_floor torch.floor_ convert_floor torch.floor_divide convert_floordiv torch.fmod convert_mod torch.gt convert_gt torch.instance_norm convert_instance_norm torch.log convert_log torch.log_ convert_log torch.lt convert_gt torch.max convert_max torch.mean convert_mean torch.min convert_min torch.mul convert_mul torch.narrow convert_narrow torch.ne convert_ne torch.neg convert_neg torch.neg_ convert_neg torch.pow convert_pow torch.prod convert_prod torch.reciprocal convert_reciprocal torch.reciprocal_ convert_reciprocal torch.relu convert_functional_relu torch.relu_ convert_functional_relu torch.selu convert_selu torch.selu_ convert_selu torch.sigmoid convert_sigmoid torch.sin convert_sin torch.sin_ convert_sin torch.sinh convert_sinh torch.sinh_ convert_sinh torch.split convert_split torch.sqrt convert_sqrt torch.sqrt_ convert_sqrt torch.squeeze convert_view torch.stack convert_cat_trt7 torch.sub convert_sub torch.sum convert_sum torch.tan convert_cos torch.tan_ convert_cos torch.tanh convert_tanh torch.tensor convert_mod torch.transpose convert_transpose_trt7 torch.unsqueeze convert_view torch.Tensor.__add__ convert_add torch.Tensor.__div__ convert_div torch.Tensor.__eq__ convert_gt torch.Tensor.__floordiv__ convert_floordiv torch.Tensor.__gt__ convert_gt torch.Tensor.__iadd__ convert_add torch.Tensor.__idiv__ convert_div torch.Tensor.__ifloordiv__ convert_floordiv torch.Tensor.__imul__ convert_mul torch.Tensor.__ipow__ convert_pow torch.Tensor.__isub__ convert_sub torch.Tensor.__itruediv__ convert_div torch.Tensor.__lt__ convert_gt torch.Tensor.__mod__ convert_mod torch.Tensor.__mul__ convert_mul torch.Tensor.__ne__ convert_ne torch.Tensor.__neg__ convert_neg torch.Tensor.__pow__ convert_pow torch.Tensor.__radd__ convert_add torch.Tensor.__rdiv__ convert_rdiv torch.Tensor.__rmul__ convert_mul torch.Tensor.__rpow__ convert_pow torch.Tensor.__rsub__ convert_sub torch.Tensor.__rtruediv__ convert_rdiv torch.Tensor.__sub__ convert_sub torch.Tensor.__truediv__ convert_div torch.Tensor.abs convert_abs torch.Tensor.abs_ convert_abs torch.Tensor.acos convert_acos torch.Tensor.acos_ convert_acos torch.Tensor.asin convert_asin torch.Tensor.asin_ convert_asin torch.Tensor.atan convert_atan torch.Tensor.atan_ convert_atan torch.Tensor.ceil convert_ceil torch.Tensor.ceil_ convert_ceil torch.Tensor.chunk convert_chunk torch.Tensor.clamp convert_clamp torch.Tensor.clamp_max convert_clamp_max torch.Tensor.clamp_min convert_clamp_min torch.Tensor.contiguous convert_functional_identity torch.Tensor.cos convert_cos torch.Tensor.cos_ convert_cos torch.Tensor.cosh convert_cosh torch.Tensor.cosh_ convert_cosh torch.Tensor.exp convert_exp torch.Tensor.exp_ convert_exp torch.Tensor.expand convert_expand torch.Tensor.floor convert_floor torch.Tensor.floor_ convert_floor torch.Tensor.log convert_log torch.Tensor.log_ convert_log torch.Tensor.max convert_max torch.Tensor.mean convert_mean torch.Tensor.min convert_min torch.Tensor.narrow convert_narrow torch.Tensor.neg convert_neg torch.Tensor.neg_ convert_neg torch.Tensor.permute convert_permute torch.Tensor.prod convert_prod torch.Tensor.reciprocal convert_reciprocal torch.Tensor.reciprocal_ convert_reciprocal torch.Tensor.relu convert_functional_relu torch.Tensor.reshape convert_view torch.Tensor.sigmoid convert_sigmoid torch.Tensor.sin convert_sin torch.Tensor.sin_ convert_sin torch.Tensor.sinh convert_sinh torch.Tensor.sinh_ convert_sinh torch.Tensor.split convert_split torch.Tensor.sqrt convert_sqrt torch.Tensor.sqrt_ convert_sqrt torch.Tensor.squeeze convert_view torch.Tensor.sum convert_sum torch.Tensor.tan convert_cos torch.Tensor.tan_ convert_cos torch.Tensor.unsqueeze convert_view torch.Tensor.view convert_view torch.nn.functional.adaptive_avg_pool2d convert_adaptive_avg_pool2d torch.nn.functional.adaptive_max_pool2d convert_adaptive_max_pool2d torch.nn.functional.avg_pool2d convert_avg_pool_trt7 torch.nn.functional.avg_pool3d convert_avg_pool_trt7 torch.nn.functional.batch_norm convert_batch_norm_trt7 torch.nn.functional.dropout convert_functional_identity torch.nn.functional.dropout2d convert_functional_identity torch.nn.functional.dropout3d convert_functional_identity torch.nn.functional.elu convert_elu torch.nn.functional.elu_ convert_elu torch.nn.functional.instance_norm convert_instance_norm torch.nn.functional.interpolate convert_interpolate_trt7 torch.nn.functional.leaky_relu convert_leaky_relu torch.nn.functional.leaky_relu_ convert_leaky_relu torch.nn.functional.max_pool2d convert_max_pool2d torch.nn.functional.normalize convert_normalize torch.nn.functional.pad convert_pad torch.nn.functional.prelu convert_prelu torch.nn.functional.relu convert_functional_relu torch.nn.functional.relu6 convert_functional_relu6 torch.nn.functional.relu_ convert_functional_relu torch.nn.functional.selu convert_selu torch.nn.functional.selu_ convert_selu torch.nn.functional.sigmoid convert_sigmoid torch.nn.functional.softmax convert_softmax torch.nn.functional.softplus convert_softplus torch.nn.functional.softsign convert_softsign torch.nn.functional.tanh convert_tanh torch.nn.functional.upsample convert_interpolate_trt7 torch.nn.AdaptiveAvgPool2d.forward convert_AdaptiveAvgPool2d torch.nn.BatchNorm1d.forward convert_BatchNorm2d torch.nn.Conv3d.forward convert_Conv_trt7 torch.nn.Conv2d.forward convert_Conv_trt7 torch.nn.Conv1d.forward convert_Conv1d torch.nn.ConvTranspose3d.forward convert_ConvTranspose2d_trt7 torch.nn.ConvTranspose2d.forward convert_ConvTranspose2d_trt7 torch.nn.Linear.forward convert_Linear torch.nn.LogSoftmax.forward convert_LogSoftmax torch.Tensor.__getitem__ convert_tensor_getitem torch.nn.Dropout3d.forward convert_identity torch.nn.Dropout2d.forward convert_identity torch.nn.Dropout.forward convert_identity torch.nn.ReLU.forward convert_relu torch.nn.ReLU6.forward convert_relu6","title":"Converters"},{"location":"getting_started.html","text":"Getting Started Follow these steps to get started using torch2trt. Note torch2trt depends on the TensorRT Python API. On Jetson, this is included with the latest JetPack. For desktop, please follow the TensorRT Installation Guide . You may also try installing torch2trt inside one of the NGC PyTorch docker containers for Desktop or Jetson . Install Without plugins To install without compiling plugins, call the following git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt python setup.py install Install With plugins To install with plugins to support some operations in PyTorch that are not natviely supported with TensorRT, call the following Note Please note, this currently only includes the interpolate plugin. This plugin requires PyTorch 1.3+ for serialization. git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt sudo python setup.py install --plugins","title":"Getting Started"},{"location":"getting_started.html#getting-started","text":"Follow these steps to get started using torch2trt. Note torch2trt depends on the TensorRT Python API. On Jetson, this is included with the latest JetPack. For desktop, please follow the TensorRT Installation Guide . You may also try installing torch2trt inside one of the NGC PyTorch docker containers for Desktop or Jetson .","title":"Getting Started"},{"location":"getting_started.html#install-without-plugins","text":"To install without compiling plugins, call the following git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt python setup.py install","title":"Install Without plugins"},{"location":"getting_started.html#install-with-plugins","text":"To install with plugins to support some operations in PyTorch that are not natviely supported with TensorRT, call the following Note Please note, this currently only includes the interpolate plugin. This plugin requires PyTorch 1.3+ for serialization. git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt sudo python setup.py install --plugins","title":"Install With plugins"},{"location":"see_also.html","text":"See Also Note The state of these converters may change over time. We provide this information here with the hope that it will help shed light on the landscape of tools available for optimizing PyTorch models with TensorRT. If you find this information helpful or outdated / misleading, please let us know. In addition to torch2trt, there are other workflows for optimizing your PyTorch model with TensorRT. The other converters we are aware of are ONNX to TensorRT Tip Since the ONNX parser ships with TensorRT, we have included a convenience method for using this workflow with torch2trt. If you want to quickly try the ONNX method using the torch2trt interface, just call torch2trt(..., use_onnx=True) . This will perform conversion on the module by exporting the model using PyTorch's JIT tracer, and parsing with TensorRT's ONNX parser. TRTorch Which one you use depends largely on your use case. The differences often come down to Layer support Modern deep learning frameworks are large, and there often arise caveats converting between frameworks using a given workflow. These could include limitations in serialization or parsing formats. Or in some instances, it may be possible the layer could be supported, but it has just not been done yet. TRTorch is strong in the sense that it will default to the original PyTorch method for layers which are not converted to TensorRT. The best way to know which conversion method works for you is to try converting your model. Feature support TensorRT is evolving and the conversion workflows may have varying level of feature support. In some instances, you may wish to use a latest feature of TensorRT, like dynamic shapes, but it is not supported in torch2trt or the interface has not yet been exposed. In this instance, we recommend checking to see if it is supported by one of the other workflows. The ONNX converter is typically strong in this regards, since the parser is distributed with TensorRT. Note If there is a TensorRT feature you wished to see in torch2trt, please let us know. We can not gaurantee this will be done, but it helps us gauge interest. Extensibility / Ease of Use In case none of the converters satisfy for your use case, you may find it necessary to adapt the converter to fit your needs. This is very intuitive with torch2trt, since it is done inline with Python, and there are many examples to reference. If you know how the original PyTorch method works, and have the TensorRT Python API on hand, it is relatively straight forward to adapt torch2trt to your needs. The extensibility is often helpful when you want to implement a converter that is specific to the context the layer appears in.","title":"See Also"},{"location":"see_also.html#see-also","text":"Note The state of these converters may change over time. We provide this information here with the hope that it will help shed light on the landscape of tools available for optimizing PyTorch models with TensorRT. If you find this information helpful or outdated / misleading, please let us know. In addition to torch2trt, there are other workflows for optimizing your PyTorch model with TensorRT. The other converters we are aware of are ONNX to TensorRT Tip Since the ONNX parser ships with TensorRT, we have included a convenience method for using this workflow with torch2trt. If you want to quickly try the ONNX method using the torch2trt interface, just call torch2trt(..., use_onnx=True) . This will perform conversion on the module by exporting the model using PyTorch's JIT tracer, and parsing with TensorRT's ONNX parser. TRTorch Which one you use depends largely on your use case. The differences often come down to","title":"See Also"},{"location":"see_also.html#layer-support","text":"Modern deep learning frameworks are large, and there often arise caveats converting between frameworks using a given workflow. These could include limitations in serialization or parsing formats. Or in some instances, it may be possible the layer could be supported, but it has just not been done yet. TRTorch is strong in the sense that it will default to the original PyTorch method for layers which are not converted to TensorRT. The best way to know which conversion method works for you is to try converting your model.","title":"Layer support"},{"location":"see_also.html#feature-support","text":"TensorRT is evolving and the conversion workflows may have varying level of feature support. In some instances, you may wish to use a latest feature of TensorRT, like dynamic shapes, but it is not supported in torch2trt or the interface has not yet been exposed. In this instance, we recommend checking to see if it is supported by one of the other workflows. The ONNX converter is typically strong in this regards, since the parser is distributed with TensorRT. Note If there is a TensorRT feature you wished to see in torch2trt, please let us know. We can not gaurantee this will be done, but it helps us gauge interest.","title":"Feature support"},{"location":"see_also.html#extensibility-ease-of-use","text":"In case none of the converters satisfy for your use case, you may find it necessary to adapt the converter to fit your needs. This is very intuitive with torch2trt, since it is done inline with Python, and there are many examples to reference. If you know how the original PyTorch method works, and have the TensorRT Python API on hand, it is relatively straight forward to adapt torch2trt to your needs. The extensibility is often helpful when you want to implement a converter that is specific to the context the layer appears in.","title":"Extensibility / Ease of Use"},{"location":"benchmarks/dla_benchmarks.html","text":"DLA Benchmarks This page presents the results of profiling resnet18 on Jetson Xavier NX under varying DLA configurations with torch2trt. Please see torch2trt/scripts/benchmark_dla.py for implementation details. Concurrent Model Execution Here we profile the throughput of resnet18 running concurrently in separate processes. Each section contains the FPS measured from each separate process running concurrently. INT8 2GPU config torch2trt_kwargs tensorrt FPS resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 331.0196110748139 resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 331.8150824541489 1GPU - 1DLA config torch2trt_kwargs tensorrt FPS resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 634.021612401935 resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 358.6787695930769 2DLA (separate cores) config torch2trt_kwargs tensorrt FPS resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 1} 422.87780423155755 resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 426.7473439918015 FP16 2GPU config torch2trt_kwargs tensorrt FPS resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 187.75452731265406 resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 186.93638306636586 1GPU - 1DLA config torch2trt_kwargs tensorrt FPS resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 355.8585787831001 resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 209.5668385056025 2DLA (separate cores) config torch2trt_kwargs tensorrt FPS resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 1} 252.11907214674176 resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 251.6072665899547 Heterogeneous Model Here we vary the precision as well as which submodules of resnet18 run on DLA vs. GPU. config torch2trt kwargs torch FPS tensorrt FPS resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1} 41.48863547383661 456.3341766145895 resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1} 41.69413020799957 274.6236549157457 resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1} 42.16469766415027 793.2889028321634 resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1} 42.02777756149902 514.262372651439 resnet18_fp16_gpu_dla1 {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA'}\", 'max_batch_size': 1} 42.7403441810802 394.57422247913195 resnet18_fp16_gpu_dla12 {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA'}\", 'max_batch_size': 1} 42.40382590738402 398.805262354432 resnet18_fp16_gpu_dla123 {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA', 'layer3': 'DLA'}\", 'max_batch_size': 1} 42.2997163615733 389.61254308033745 resnet18_int8_gpu_dla1 {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA'}\", 'max_batch_size': 1} 42.623790888121285 476.278066006966 resnet18_int8_gpu_dla12 {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA'}\", 'max_batch_size': 1} 42.320715371630605 496.7642812355515 resnet18_int8_gpu_dla123 {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA', 'layer3': 'DLA'}\", 'max_batch_size': 1} 41.859373689253914 500.02851665716946","title":"DLA Benchmarks"},{"location":"benchmarks/dla_benchmarks.html#dla-benchmarks","text":"This page presents the results of profiling resnet18 on Jetson Xavier NX under varying DLA configurations with torch2trt. Please see torch2trt/scripts/benchmark_dla.py for implementation details.","title":"DLA Benchmarks"},{"location":"benchmarks/dla_benchmarks.html#concurrent-model-execution","text":"Here we profile the throughput of resnet18 running concurrently in separate processes. Each section contains the FPS measured from each separate process running concurrently.","title":"Concurrent Model Execution"},{"location":"benchmarks/dla_benchmarks.html#int8","text":"","title":"INT8"},{"location":"benchmarks/dla_benchmarks.html#2gpu","text":"config torch2trt_kwargs tensorrt FPS resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 331.0196110748139 resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 331.8150824541489","title":"2GPU"},{"location":"benchmarks/dla_benchmarks.html#1gpu-1dla","text":"config torch2trt_kwargs tensorrt FPS resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 634.021612401935 resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 358.6787695930769","title":"1GPU - 1DLA"},{"location":"benchmarks/dla_benchmarks.html#2dla-separate-cores","text":"config torch2trt_kwargs tensorrt FPS resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 1} 422.87780423155755 resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 426.7473439918015","title":"2DLA (separate cores)"},{"location":"benchmarks/dla_benchmarks.html#fp16","text":"","title":"FP16"},{"location":"benchmarks/dla_benchmarks.html#2gpu_1","text":"config torch2trt_kwargs tensorrt FPS resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 187.75452731265406 resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 186.93638306636586","title":"2GPU"},{"location":"benchmarks/dla_benchmarks.html#1gpu-1dla_1","text":"config torch2trt_kwargs tensorrt FPS resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1, 'dla_core': 0} 355.8585787831001 resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 209.5668385056025","title":"1GPU - 1DLA"},{"location":"benchmarks/dla_benchmarks.html#2dla-separate-cores_1","text":"config torch2trt_kwargs tensorrt FPS resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 1} 252.11907214674176 resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1, 'dla_core': 0} 251.6072665899547","title":"2DLA (separate cores)"},{"location":"benchmarks/dla_benchmarks.html#heterogeneous-model","text":"Here we vary the precision as well as which submodules of resnet18 run on DLA vs. GPU. config torch2trt kwargs torch FPS tensorrt FPS resnet18_fp16_gpu {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1} 41.48863547383661 456.3341766145895 resnet18_fp16_dla {'fp16_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1} 41.69413020799957 274.6236549157457 resnet18_int8_gpu {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'max_batch_size': 1} 42.16469766415027 793.2889028321634 resnet18_int8_dla {'int8_mode': True, 'default_device_type': DeviceType.DLA, 'max_batch_size': 1} 42.02777756149902 514.262372651439 resnet18_fp16_gpu_dla1 {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA'}\", 'max_batch_size': 1} 42.7403441810802 394.57422247913195 resnet18_fp16_gpu_dla12 {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA'}\", 'max_batch_size': 1} 42.40382590738402 398.805262354432 resnet18_fp16_gpu_dla123 {'fp16_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA', 'layer3': 'DLA'}\", 'max_batch_size': 1} 42.2997163615733 389.61254308033745 resnet18_int8_gpu_dla1 {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA'}\", 'max_batch_size': 1} 42.623790888121285 476.278066006966 resnet18_int8_gpu_dla12 {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA'}\", 'max_batch_size': 1} 42.320715371630605 496.7642812355515 resnet18_int8_gpu_dla123 {'int8_mode': True, 'default_device_type': DeviceType.GPU, 'device_types': \"{'layer1': 'DLA', 'layer2': 'DLA', 'layer3': 'DLA'}\", 'max_batch_size': 1} 41.859373689253914 500.02851665716946","title":"Heterogeneous Model"},{"location":"benchmarks/jetson_nano.html","text":"Jetson Nano Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torchvision.models.alexnet.alexnet float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.29E-05 46.4 69.9 22.1 14.7 torchvision.models.squeezenet.squeezenet1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.20E-02 44 137 24.2 7.6 torchvision.models.squeezenet.squeezenet1_1 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 76.6 248 14 4.34 torchvision.models.resnet.resnet18 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 5.86E-03 29.4 90.2 34.7 11.4 torchvision.models.resnet.resnet34 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.56E-01 15.5 50.7 64.8 20.2 torchvision.models.resnet.resnet50 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 6.45E-02 12.4 34.2 81.7 29.8 torchvision.models.resnet.resnet101 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.01E+03 7.18 19.9 141 51.1 torchvision.models.resnet.resnet152 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 4.96 14.1 204 72.3 torchvision.models.densenet.densenet121 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.42E-03 11.5 41.9 84.5 24.8 torchvision.models.densenet.densenet169 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 5.86E-03 8.25 33.2 118 31.2 torchvision.models.densenet.densenet201 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.42E-03 6.84 25.4 141 40.8 torchvision.models.densenet.densenet161 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.15E-03 4.71 15.6 247 65.8 torchvision.models.vgg.vgg11 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.51E-04 8.9 18.3 114 55.1 torchvision.models.vgg.vgg13 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.07E-04 6.53 14.7 156 68.7 torchvision.models.vgg.vgg16 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.58E-04 5.09 11.9 201 85.1 torchvision.models.vgg.vgg11_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.81E-04 8.74 18.4 117 54.8 torchvision.models.vgg.vgg13_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 5.19E-04 6.31 14.8 162 68.5 torchvision.models.vgg.vgg16_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 4.96 12 207 84.3","title":"Jetson Nano"},{"location":"benchmarks/jetson_nano.html#jetson-nano","text":"Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torchvision.models.alexnet.alexnet float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.29E-05 46.4 69.9 22.1 14.7 torchvision.models.squeezenet.squeezenet1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.20E-02 44 137 24.2 7.6 torchvision.models.squeezenet.squeezenet1_1 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 76.6 248 14 4.34 torchvision.models.resnet.resnet18 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 5.86E-03 29.4 90.2 34.7 11.4 torchvision.models.resnet.resnet34 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.56E-01 15.5 50.7 64.8 20.2 torchvision.models.resnet.resnet50 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 6.45E-02 12.4 34.2 81.7 29.8 torchvision.models.resnet.resnet101 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.01E+03 7.18 19.9 141 51.1 torchvision.models.resnet.resnet152 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 4.96 14.1 204 72.3 torchvision.models.densenet.densenet121 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.42E-03 11.5 41.9 84.5 24.8 torchvision.models.densenet.densenet169 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 5.86E-03 8.25 33.2 118 31.2 torchvision.models.densenet.densenet201 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.42E-03 6.84 25.4 141 40.8 torchvision.models.densenet.densenet161 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.15E-03 4.71 15.6 247 65.8 torchvision.models.vgg.vgg11 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.51E-04 8.9 18.3 114 55.1 torchvision.models.vgg.vgg13 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.07E-04 6.53 14.7 156 68.7 torchvision.models.vgg.vgg16 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.58E-04 5.09 11.9 201 85.1 torchvision.models.vgg.vgg11_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.81E-04 8.74 18.4 117 54.8 torchvision.models.vgg.vgg13_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 5.19E-04 6.31 14.8 162 68.5 torchvision.models.vgg.vgg16_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 4.96 12 207 84.3","title":"Jetson Nano"},{"location":"benchmarks/jetson_xavier.html","text":"Jetson Xavier Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torch2trt.tests.torchvision.classification.alexnet float16 [(1, 3, 224, 224)] {'fp16_mode': True} 7.63E-05 251 565 4.96 2.02 torch2trt.tests.torchvision.classification.squeezenet1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 121 834 8.04 1.49 torch2trt.tests.torchvision.classification.squeezenet1_1 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 125 1.29e+03 8.01 1.02 torch2trt.tests.torchvision.classification.resnet18 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-03 136 722 7.33 1.64 torch2trt.tests.torchvision.classification.resnet34 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.50E-01 77.8 396 12.9 2.79 torch2trt.tests.torchvision.classification.resnet50 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.09E-01 55.8 326 17.9 3.37 torch2trt.tests.torchvision.classification.resnet101 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 28.3 175 35.1 6.04 torch2trt.tests.torchvision.classification.resnet152 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 18.8 122 53.2 8.57 torch2trt.tests.torchvision.classification.densenet121 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 7.81E-03 20.9 76.6 47.5 13 torch2trt.tests.torchvision.classification.densenet169 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.91E-03 14.8 41.7 66.7 23.7 torch2trt.tests.torchvision.classification.densenet201 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.88E-03 12.6 30.2 79.1 33 torch2trt.tests.torchvision.classification.densenet161 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.88E-03 16.1 43.7 62.1 23 torch2trt.tests.torchvision.classification.vgg11 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.56E-03 84.8 201 12.1 5.24 torch2trt.tests.torchvision.classification.vgg13 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.24E-03 71.1 165 14.3 6.34 torch2trt.tests.torchvision.classification.vgg16 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.78E-03 61.5 139 16.5 7.46 torch2trt.tests.torchvision.classification.vgg19 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.81E-03 54.1 120 18.7 8.61 torch2trt.tests.torchvision.classification.vgg11_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.20E-03 81.5 200 12.5 5.27 torch2trt.tests.torchvision.classification.vgg13_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.71E-03 67.5 165 15.1 6.33 torch2trt.tests.torchvision.classification.vgg16_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.87E-03 58.3 139 17.4 7.48 torch2trt.tests.torchvision.classification.vgg19_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.44E-03 51.4 120 19.7 8.61 torch2trt.tests.torchvision.classification.mobilenet_v2 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 64.8 723 15.4 1.67 torch2trt.tests.torchvision.classification.shufflenet_v2_x0_5 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 51.2 463 19.4 2.17 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 49.4 419 20.4 2.43 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_5 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 51.4 426 19.6 2.37 torch2trt.tests.torchvision.classification.shufflenet_v2_x2_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 48.2 419 20.8 2.48 torch2trt.tests.torchvision.classification.mnasnet0_5 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.03E-06 67.8 883 14.9 1.4 torch2trt.tests.torchvision.classification.mnasnet0_75 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 67.6 751 14.8 1.6 torch2trt.tests.torchvision.classification.mnasnet1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 65.7 667 15.2 1.77 torch2trt.tests.torchvision.classification.mnasnet1_3 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 67.4 573 15 2.02","title":"Jetson Xavier"},{"location":"benchmarks/jetson_xavier.html#jetson-xavier","text":"Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torch2trt.tests.torchvision.classification.alexnet float16 [(1, 3, 224, 224)] {'fp16_mode': True} 7.63E-05 251 565 4.96 2.02 torch2trt.tests.torchvision.classification.squeezenet1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 121 834 8.04 1.49 torch2trt.tests.torchvision.classification.squeezenet1_1 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-04 125 1.29e+03 8.01 1.02 torch2trt.tests.torchvision.classification.resnet18 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 9.77E-03 136 722 7.33 1.64 torch2trt.tests.torchvision.classification.resnet34 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.50E-01 77.8 396 12.9 2.79 torch2trt.tests.torchvision.classification.resnet50 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.09E-01 55.8 326 17.9 3.37 torch2trt.tests.torchvision.classification.resnet101 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 28.3 175 35.1 6.04 torch2trt.tests.torchvision.classification.resnet152 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 18.8 122 53.2 8.57 torch2trt.tests.torchvision.classification.densenet121 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 7.81E-03 20.9 76.6 47.5 13 torch2trt.tests.torchvision.classification.densenet169 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.91E-03 14.8 41.7 66.7 23.7 torch2trt.tests.torchvision.classification.densenet201 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.88E-03 12.6 30.2 79.1 33 torch2trt.tests.torchvision.classification.densenet161 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 4.88E-03 16.1 43.7 62.1 23 torch2trt.tests.torchvision.classification.vgg11 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.56E-03 84.8 201 12.1 5.24 torch2trt.tests.torchvision.classification.vgg13 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.24E-03 71.1 165 14.3 6.34 torch2trt.tests.torchvision.classification.vgg16 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 3.78E-03 61.5 139 16.5 7.46 torch2trt.tests.torchvision.classification.vgg19 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.81E-03 54.1 120 18.7 8.61 torch2trt.tests.torchvision.classification.vgg11_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.20E-03 81.5 200 12.5 5.27 torch2trt.tests.torchvision.classification.vgg13_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.71E-03 67.5 165 15.1 6.33 torch2trt.tests.torchvision.classification.vgg16_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.87E-03 58.3 139 17.4 7.48 torch2trt.tests.torchvision.classification.vgg19_bn float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.44E-03 51.4 120 19.7 8.61 torch2trt.tests.torchvision.classification.mobilenet_v2 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 64.8 723 15.4 1.67 torch2trt.tests.torchvision.classification.shufflenet_v2_x0_5 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 51.2 463 19.4 2.17 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 49.4 419 20.4 2.43 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_5 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 51.4 426 19.6 2.37 torch2trt.tests.torchvision.classification.shufflenet_v2_x2_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 1.53E-05 48.2 419 20.8 2.48 torch2trt.tests.torchvision.classification.mnasnet0_5 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 2.03E-06 67.8 883 14.9 1.4 torch2trt.tests.torchvision.classification.mnasnet0_75 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 67.6 751 14.8 1.6 torch2trt.tests.torchvision.classification.mnasnet1_0 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 65.7 667 15.2 1.77 torch2trt.tests.torchvision.classification.mnasnet1_3 float16 [(1, 3, 224, 224)] {'fp16_mode': True} 0.00E+00 67.4 573 15 2.02","title":"Jetson Xavier"},{"location":"usage/basic_usage.html","text":"Basic Usage This page demonstrates basic torch2trt usage. Conversion You can easily convert a PyTorch module by calling torch2trt passing example data as input, for example to convert alexnet we call import torch from torch2trt import torch2trt from torchvision.models.alexnet import alexnet # create some regular pytorch model... model = alexnet ( pretrained = True ) . eval () . cuda () # create example data x = torch . ones (( 1 , 3 , 224 , 224 )) . cuda () # convert to TensorRT feeding sample data as input model_trt = torch2trt ( model , [ x ]) Note Currently with torch2trt, once the model is converted, you must use the same input shapes during execution. The exception is the batch size, which can vary up to the value specified by the max_batch_size parameter. Executution We can execute the returned TRTModule just like the original PyTorch model. Here we execute the model and print the maximum absolute error. y = model ( x ) y_trt = model_trt ( x ) # check the output against PyTorch print ( torch . max ( torch . abs ( y - y_trt ))) Saving and loading We can save the model as a state_dict . torch . save ( model_trt . state_dict (), 'alexnet_trt.pth' ) We can load the saved model into a TRTModule from torch2trt import TRTModule model_trt = TRTModule () model_trt . load_state_dict ( torch . load ( 'alexnet_trt.pth' ))","title":"Basic Usage"},{"location":"usage/basic_usage.html#basic-usage","text":"This page demonstrates basic torch2trt usage.","title":"Basic Usage"},{"location":"usage/basic_usage.html#conversion","text":"You can easily convert a PyTorch module by calling torch2trt passing example data as input, for example to convert alexnet we call import torch from torch2trt import torch2trt from torchvision.models.alexnet import alexnet # create some regular pytorch model... model = alexnet ( pretrained = True ) . eval () . cuda () # create example data x = torch . ones (( 1 , 3 , 224 , 224 )) . cuda () # convert to TensorRT feeding sample data as input model_trt = torch2trt ( model , [ x ]) Note Currently with torch2trt, once the model is converted, you must use the same input shapes during execution. The exception is the batch size, which can vary up to the value specified by the max_batch_size parameter.","title":"Conversion"},{"location":"usage/basic_usage.html#executution","text":"We can execute the returned TRTModule just like the original PyTorch model. Here we execute the model and print the maximum absolute error. y = model ( x ) y_trt = model_trt ( x ) # check the output against PyTorch print ( torch . max ( torch . abs ( y - y_trt )))","title":"Executution"},{"location":"usage/basic_usage.html#saving-and-loading","text":"We can save the model as a state_dict . torch . save ( model_trt . state_dict (), 'alexnet_trt.pth' ) We can load the saved model into a TRTModule from torch2trt import TRTModule model_trt = TRTModule () model_trt . load_state_dict ( torch . load ( 'alexnet_trt.pth' ))","title":"Saving and loading"},{"location":"usage/custom_converter.html","text":"Custom Converter This page details how to extend or modify the behavior of torch2trt by implementing and registering custom converters. Background torch2trt works by attaching conversion functions (like convert_ReLU ) to the original PyTorch functional calls (like torch.nn.ReLU.forward ). The sample input data is passed through the network, just as before, except now whenever a registered function ( torch.nn.ReLU.forward ) is encountered, the corresponding converter ( convert_ReLU ) is also called afterwards. The converter is passed the arguments and return statement of the original PyTorch function, as well as the TensorRT network that is being constructed. The input tensors to the original PyTorch function are modified to have an attribute _trt , which is the TensorRT counterpart to the PyTorch tensor. The conversion function uses this _trt to add layers to the TensorRT network, and then sets the _trt attribute for relevant output tensors. Once the model is fully executed, the final tensors returns are marked as outputs of the TensorRT network, and the optimized TensorRT engine is built. Add a custom converter Here we show how to add a converter for the ReLU module using the TensorRT python API. import tensorrt as trt from torch2trt import tensorrt_converter @tensorrt_converter ( 'torch.nn.ReLU.forward' ) def convert_ReLU ( ctx ): input = ctx . method_args [ 1 ] output = ctx . method_return layer = ctx . network . add_activation ( input = input . _trt , type = trt . ActivationType . RELU ) output . _trt = layer . get_output ( 0 ) The converter takes one argument, a ConversionContext , which will contain the following ctx.network - The TensorRT network that is being constructed. ctx.method_args - Positional arguments that were passed to the specified PyTorch function. The _trt attribute is set for relevant input tensors. ctx.method_kwargs - Keyword arguments that were passed to the specified PyTorch function. ctx.method_return - The value returned by the specified PyTorch function. The converter must set the _trt attribute where relevant. Please see the converters page for a list of implemented converters and links to their source code. These may help in learning how to write converters.","title":"Custom Converter"},{"location":"usage/custom_converter.html#custom-converter","text":"This page details how to extend or modify the behavior of torch2trt by implementing and registering custom converters.","title":"Custom Converter"},{"location":"usage/custom_converter.html#background","text":"torch2trt works by attaching conversion functions (like convert_ReLU ) to the original PyTorch functional calls (like torch.nn.ReLU.forward ). The sample input data is passed through the network, just as before, except now whenever a registered function ( torch.nn.ReLU.forward ) is encountered, the corresponding converter ( convert_ReLU ) is also called afterwards. The converter is passed the arguments and return statement of the original PyTorch function, as well as the TensorRT network that is being constructed. The input tensors to the original PyTorch function are modified to have an attribute _trt , which is the TensorRT counterpart to the PyTorch tensor. The conversion function uses this _trt to add layers to the TensorRT network, and then sets the _trt attribute for relevant output tensors. Once the model is fully executed, the final tensors returns are marked as outputs of the TensorRT network, and the optimized TensorRT engine is built.","title":"Background"},{"location":"usage/custom_converter.html#add-a-custom-converter","text":"Here we show how to add a converter for the ReLU module using the TensorRT python API. import tensorrt as trt from torch2trt import tensorrt_converter @tensorrt_converter ( 'torch.nn.ReLU.forward' ) def convert_ReLU ( ctx ): input = ctx . method_args [ 1 ] output = ctx . method_return layer = ctx . network . add_activation ( input = input . _trt , type = trt . ActivationType . RELU ) output . _trt = layer . get_output ( 0 ) The converter takes one argument, a ConversionContext , which will contain the following ctx.network - The TensorRT network that is being constructed. ctx.method_args - Positional arguments that were passed to the specified PyTorch function. The _trt attribute is set for relevant input tensors. ctx.method_kwargs - Keyword arguments that were passed to the specified PyTorch function. ctx.method_return - The value returned by the specified PyTorch function. The converter must set the _trt attribute where relevant. Please see the converters page for a list of implemented converters and links to their source code. These may help in learning how to write converters.","title":"Add a custom converter"},{"location":"usage/dla_usage.html","text":"DLA Usage Some NVIDIA devices, like Jetson Xavier NX and Jetson Xavier AGX, come equipped with one or more Deep Learning Accelerators (DLA). These are dedicated hardware units which are capable of processing a variety of common neural network functions. Interfacing with these devices is accomplished through TensorRT. This page details how you can utilize the DLA(s) on your device with torch2trt. Note We've benchmarked resnet18 running on Jetson Xavier NX under varying configurations. You can find the results here . This may help you understand if DLA is applicable for your use case. Enable DLA Globally Enabling the DLA globally is done by passing default_device_type=trt.DeviceType.DLA to torch2trt. If a layer is not supported on DLA, it will default to GPU. Please note, DLA only supports FP16 and INT8 precision, so you must also set either fp16_mode=True or int8_mode=True . To use DLA with FP16 precision you would call the following. import tensorrt as trt # used to access trt.DeviceType enumeration from torch2trt import torch2trt model_trt = torch2trt ( model , [ data ], fp16_mode = True , default_device_type = trt . DeviceType . DLA ) Similarily, you could easily use int8_mode instead by calling model_trt = torch2trt ( model , [ data ], int8_mode = True , default_device_type = trt . DeviceType . DLA ) INT8 will require calibration to compute the dynamic ranges needed for quantization. For more details on this, see the page on reduced precision Specify DLA submodules torch2trt allows you to explicitly control the granularity of which submodules should run on DLA and which should run on GPU. This is accomplished through the device_types argument. This argument takes a dictionary mapping pytorch modules to TensorRT device types. For example, we can convert the resnet18 blocks layer1 and layer2 to run on DLA. model = resnet18 ( pretrained = True ) . cuda () . eval () device_types = { model . layer1 : trt . DeviceType . DLA , model . layer2 : trt . DeviceType . DLA } model_trt = torch2trt ( model , [ data ], fp16_mode = True , device_types = device_types ) If we wanted, we could override the first block in layer2 back to GPU, by specifying the device type for this child element. This would look like the following, device_types = { model . layer1 : trt . DeviceType . DLA , model . layer2 : trt . DeviceType . DLA , model . layer2 [ 0 ]: trt . DeviceType . GPU } ... Device types will default to default_device_type if a parent module is not specified in device_types . By default, default_device_type is set to trt.DeviceType.GPU . If you wanted to enable DLA globally, and selectively set layers to run on GPU, you could do the following. model = resnet18 ( pretrained = True ) . cuda () . eval () device_types = { model . layer1 : trt . DeviceType . GPU , model . layer2 : trt . DeviceType . GPU } model_trt = torch2trt ( model , [ data ], default_device_typee = trt . DeviceType . DLA , fp16_mode = True , device_types = device_types ) Specify DLA core For devices with multiple DLA cores, you may set the dla_core attribute to control which DLA core the TensorRT engine should run on. This will apply globally to all layers using DLA inside the converted module. For example, to use DLA core 1 we would call, model_trt = torch2trt ( model , [ data ], dla_core = 1 , ... ) Info Currently, the TensorRT Python API doesn't support setting the DLA core at runtime. For this reason, models which are serialized / deserialized will default to DLA core 0. However, the model returned immediately from torch2trt will use the DLA set by the dla_core argument.","title":"DLA Usage"},{"location":"usage/dla_usage.html#dla-usage","text":"Some NVIDIA devices, like Jetson Xavier NX and Jetson Xavier AGX, come equipped with one or more Deep Learning Accelerators (DLA). These are dedicated hardware units which are capable of processing a variety of common neural network functions. Interfacing with these devices is accomplished through TensorRT. This page details how you can utilize the DLA(s) on your device with torch2trt. Note We've benchmarked resnet18 running on Jetson Xavier NX under varying configurations. You can find the results here . This may help you understand if DLA is applicable for your use case.","title":"DLA Usage"},{"location":"usage/dla_usage.html#enable-dla-globally","text":"Enabling the DLA globally is done by passing default_device_type=trt.DeviceType.DLA to torch2trt. If a layer is not supported on DLA, it will default to GPU. Please note, DLA only supports FP16 and INT8 precision, so you must also set either fp16_mode=True or int8_mode=True . To use DLA with FP16 precision you would call the following. import tensorrt as trt # used to access trt.DeviceType enumeration from torch2trt import torch2trt model_trt = torch2trt ( model , [ data ], fp16_mode = True , default_device_type = trt . DeviceType . DLA ) Similarily, you could easily use int8_mode instead by calling model_trt = torch2trt ( model , [ data ], int8_mode = True , default_device_type = trt . DeviceType . DLA ) INT8 will require calibration to compute the dynamic ranges needed for quantization. For more details on this, see the page on reduced precision","title":"Enable DLA Globally"},{"location":"usage/dla_usage.html#specify-dla-submodules","text":"torch2trt allows you to explicitly control the granularity of which submodules should run on DLA and which should run on GPU. This is accomplished through the device_types argument. This argument takes a dictionary mapping pytorch modules to TensorRT device types. For example, we can convert the resnet18 blocks layer1 and layer2 to run on DLA. model = resnet18 ( pretrained = True ) . cuda () . eval () device_types = { model . layer1 : trt . DeviceType . DLA , model . layer2 : trt . DeviceType . DLA } model_trt = torch2trt ( model , [ data ], fp16_mode = True , device_types = device_types ) If we wanted, we could override the first block in layer2 back to GPU, by specifying the device type for this child element. This would look like the following, device_types = { model . layer1 : trt . DeviceType . DLA , model . layer2 : trt . DeviceType . DLA , model . layer2 [ 0 ]: trt . DeviceType . GPU } ... Device types will default to default_device_type if a parent module is not specified in device_types . By default, default_device_type is set to trt.DeviceType.GPU . If you wanted to enable DLA globally, and selectively set layers to run on GPU, you could do the following. model = resnet18 ( pretrained = True ) . cuda () . eval () device_types = { model . layer1 : trt . DeviceType . GPU , model . layer2 : trt . DeviceType . GPU } model_trt = torch2trt ( model , [ data ], default_device_typee = trt . DeviceType . DLA , fp16_mode = True , device_types = device_types )","title":"Specify DLA submodules"},{"location":"usage/dla_usage.html#specify-dla-core","text":"For devices with multiple DLA cores, you may set the dla_core attribute to control which DLA core the TensorRT engine should run on. This will apply globally to all layers using DLA inside the converted module. For example, to use DLA core 1 we would call, model_trt = torch2trt ( model , [ data ], dla_core = 1 , ... ) Info Currently, the TensorRT Python API doesn't support setting the DLA core at runtime. For this reason, models which are serialized / deserialized will default to DLA core 0. However, the model returned immediately from torch2trt will use the DLA set by the dla_core argument.","title":"Specify DLA core"},{"location":"usage/reduced_precision.html","text":"Reduced Precision For certain platforms, reduced precision can result in substantial improvements in throughput, often with little impact on model accuracy. Support Matrix Below is a table of layer precision support for various NVIDIA platforms. Platform FP16 INT8 Jetson Nano Jetson TX2 Jetson Xavier NX Jetson AGX Xavier Note If the platform you're using is missing from this table or you spot anything incorrect please let us know . FP16 Precision To enable support for fp16 precision with TensorRT, torch2trt exposes the fp16_mode parameter. Converting a model with fp16_mode=True allows the TensorRT optimizer to select layers with fp16 precision. model_trt = torch2trt ( model , [ data ], fp16_mode = True ) Note When fp16_mode=True , this does not necessarily mean that TensorRT will select FP16 layers. The optimizer attempts to automatically select tactics which result in the best performance. INT8 Precision torch2trt also supports int8 precision with TensorRT with the int8_mode parameter. Unlike fp16 and fp32 precision, switching to in8 precision often requires calibration to avoid a significant drop in accuracy. Input Data Calibration By default torch2trt will calibrate using the input data provided. For example, if you wanted to calibrate on a set of 64 random normal images you could do. data = torch . randn ( 64 , 3 , 224 , 224 ) . cuda () . eval () model_trt = torch2trt ( model , [ data ], int8_mode = True ) Dataset Calibration In many instances, you may want to calibrate on more data than fits in memory. For this reason, torch2trt exposes the int8_calibration_dataset parameter. This parameter takes an input dataset that is used for calibration. If this parameter is specified, the input data is ignored during calibration. You create an input dataset by defining a class which implements the __len__ and __getitem__ methods. The __len__ method should return the number of calibration samples The __getitem__ method must return a single calibration sample. This is a list of input tensors to the model. Each tensor should match the shape you provide to the inputs parameter when calling torch2trt . For example, say you trained an image classification network using the PyTorch ImageFolder dataset. You could wrap this dataset for calibration, by defining a new dataset which returns only the images without labels in list format. from torchvision.datasets import ImageFolder from torchvision.transforms import ToTensor , Compose , Normalize , Resize class ImageFolderCalibDataset (): def __init__ ( self , root ): self . dataset = ImageFolder ( root = root , transform = Compose ([ Resize (( 224 , 224 )), ToTensor (), Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) ) def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx ): image , _ = self . dataset [ idx ] image = image [ None , ... ] # add batch dimension return [ image ] You would then provide this calibration dataset to torch2trt as follows dataset = ImageFolderCalibDataset ( 'images' ) model_trt = torch2trt ( model , [ data ], int8_calib_dataset = dataset ) Calibration Algorithm To override the default calibration algorithm that torch2trt uses, you can set the int8_calib_algoirthm to the tensorrt.CalibrationAlgoType that you wish to use. For example, to use the minmax calibration algorithm you would do import tensorrt as trt model_trt = torch2trt ( model , [ data ], int8_mode = True , int8_calib_algorithm = trt . CalibrationAlgoType . MINMAX_CALIBRATION ) Calibration Batch Size During calibration, torch2trt pulls data in batches for the TensorRT calibrator. In some instances developers have found that the calibration batch size can impact the calibrated model accuracy. To set the calibration batch size, you can set the int8_calib_batch_size parameter. For example, to use a calibration batch size of 32 you could do model_trt = torch2trt ( model , [ data ], int8_mode = True , int8_calib_batch_size = 32 ) Binding Data Types The data type of input and output bindings in TensorRT are determined by the original PyTorch module input and output data types. This does not directly impact whether the TensorRT optimizer will internally use fp16 or int8 precision. For example, to create a model with fp32 precision bindings, you would do the following model = model . float () data = data . float () model_trt = torch2trt ( model , [ data ], fp16_mode = True ) In this instance, the optimizer may choose to use fp16 precision layers internally, but the input and output data types are fp32. To use fp16 precision input and output bindings you would do model = model . half () data = data . half () model_trt = torch2trt ( model , [ data ], fp16_mode = True ) Now, the input and output bindings of the model are half precision, and internally the optimizer may choose to select fp16 layers as well.","title":"Reduced Precision"},{"location":"usage/reduced_precision.html#reduced-precision","text":"For certain platforms, reduced precision can result in substantial improvements in throughput, often with little impact on model accuracy.","title":"Reduced Precision"},{"location":"usage/reduced_precision.html#support-matrix","text":"Below is a table of layer precision support for various NVIDIA platforms. Platform FP16 INT8 Jetson Nano Jetson TX2 Jetson Xavier NX Jetson AGX Xavier Note If the platform you're using is missing from this table or you spot anything incorrect please let us know .","title":"Support Matrix"},{"location":"usage/reduced_precision.html#fp16-precision","text":"To enable support for fp16 precision with TensorRT, torch2trt exposes the fp16_mode parameter. Converting a model with fp16_mode=True allows the TensorRT optimizer to select layers with fp16 precision. model_trt = torch2trt ( model , [ data ], fp16_mode = True ) Note When fp16_mode=True , this does not necessarily mean that TensorRT will select FP16 layers. The optimizer attempts to automatically select tactics which result in the best performance.","title":"FP16 Precision"},{"location":"usage/reduced_precision.html#int8-precision","text":"torch2trt also supports int8 precision with TensorRT with the int8_mode parameter. Unlike fp16 and fp32 precision, switching to in8 precision often requires calibration to avoid a significant drop in accuracy.","title":"INT8 Precision"},{"location":"usage/reduced_precision.html#input-data-calibration","text":"By default torch2trt will calibrate using the input data provided. For example, if you wanted to calibrate on a set of 64 random normal images you could do. data = torch . randn ( 64 , 3 , 224 , 224 ) . cuda () . eval () model_trt = torch2trt ( model , [ data ], int8_mode = True )","title":"Input Data Calibration"},{"location":"usage/reduced_precision.html#dataset-calibration","text":"In many instances, you may want to calibrate on more data than fits in memory. For this reason, torch2trt exposes the int8_calibration_dataset parameter. This parameter takes an input dataset that is used for calibration. If this parameter is specified, the input data is ignored during calibration. You create an input dataset by defining a class which implements the __len__ and __getitem__ methods. The __len__ method should return the number of calibration samples The __getitem__ method must return a single calibration sample. This is a list of input tensors to the model. Each tensor should match the shape you provide to the inputs parameter when calling torch2trt . For example, say you trained an image classification network using the PyTorch ImageFolder dataset. You could wrap this dataset for calibration, by defining a new dataset which returns only the images without labels in list format. from torchvision.datasets import ImageFolder from torchvision.transforms import ToTensor , Compose , Normalize , Resize class ImageFolderCalibDataset (): def __init__ ( self , root ): self . dataset = ImageFolder ( root = root , transform = Compose ([ Resize (( 224 , 224 )), ToTensor (), Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]) ) def __len__ ( self ): return len ( self . dataset ) def __getitem__ ( self , idx ): image , _ = self . dataset [ idx ] image = image [ None , ... ] # add batch dimension return [ image ] You would then provide this calibration dataset to torch2trt as follows dataset = ImageFolderCalibDataset ( 'images' ) model_trt = torch2trt ( model , [ data ], int8_calib_dataset = dataset )","title":"Dataset Calibration"},{"location":"usage/reduced_precision.html#calibration-algorithm","text":"To override the default calibration algorithm that torch2trt uses, you can set the int8_calib_algoirthm to the tensorrt.CalibrationAlgoType that you wish to use. For example, to use the minmax calibration algorithm you would do import tensorrt as trt model_trt = torch2trt ( model , [ data ], int8_mode = True , int8_calib_algorithm = trt . CalibrationAlgoType . MINMAX_CALIBRATION )","title":"Calibration Algorithm"},{"location":"usage/reduced_precision.html#calibration-batch-size","text":"During calibration, torch2trt pulls data in batches for the TensorRT calibrator. In some instances developers have found that the calibration batch size can impact the calibrated model accuracy. To set the calibration batch size, you can set the int8_calib_batch_size parameter. For example, to use a calibration batch size of 32 you could do model_trt = torch2trt ( model , [ data ], int8_mode = True , int8_calib_batch_size = 32 )","title":"Calibration Batch Size"},{"location":"usage/reduced_precision.html#binding-data-types","text":"The data type of input and output bindings in TensorRT are determined by the original PyTorch module input and output data types. This does not directly impact whether the TensorRT optimizer will internally use fp16 or int8 precision. For example, to create a model with fp32 precision bindings, you would do the following model = model . float () data = data . float () model_trt = torch2trt ( model , [ data ], fp16_mode = True ) In this instance, the optimizer may choose to use fp16 precision layers internally, but the input and output data types are fp32. To use fp16 precision input and output bindings you would do model = model . half () data = data . half () model_trt = torch2trt ( model , [ data ], fp16_mode = True ) Now, the input and output bindings of the model are half precision, and internally the optimizer may choose to select fp16 layers as well.","title":"Binding Data Types"}]}