{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"torch2trt torch2trt is a PyTorch to TensorRT converter which utilizes the TensorRT Python API. The converter is Easy to use - Convert modules with a single function call torch2trt Easy to extend - Write your own layer converter in Python and register it with @tensorrt_converter If you find an issue, please let us know !","title":"Home"},{"location":"index.html#torch2trt","text":"torch2trt is a PyTorch to TensorRT converter which utilizes the TensorRT Python API. The converter is Easy to use - Convert modules with a single function call torch2trt Easy to extend - Write your own layer converter in Python and register it with @tensorrt_converter If you find an issue, please let us know !","title":"torch2trt"},{"location":"CHANGELOG.html","text":"Changes","title":"Changes"},{"location":"CHANGELOG.html#changes","text":"","title":"Changes"},{"location":"CONTRIBUTING.html","text":"Contributing Forms of contribution Submit an Issue torch2trt is use case driven. We originally created it to solve use cases related to NVIDIA Jetson, but the layer support has grown largely since it's release and we've found that it has helped many other developers as well. The growth of torch2trt has been largely driven by issues submitted on GitHub . We learn a lot from the reported issues. Submitting an issue it is one of the best ways to begin contributing to torch2trt. The reported issues typically are one of the following, A bug or unexpected result A model with unsupported layers If you report an issue, we typically find the following information helpful PyTorch version TensorRT version Platform (ie: Jetson Nano) The PyTorch Module you're attempting to convert The steps taken to convert the PyTorch module If you're not sure how to provide any of these pieces of information, don't worry. Just open the issue and we're happy to discuss and help work out the details. Ask a Question Another great way to contribute is to ask a question on GitHub . There are often other developers who share your question, and they may find the discussion helpful. This also helps us gauge feature interest and identify gaps in documentation. Submit a Pull Request torch2trt is use case driven and has limited maintainence, for this reason we value community contributions greatly. Another great way to contribute is by submitting a pull request. Pull requests which are most likely to be accepted are A new converter A test case A bug fix If you add a new converter, it is best to include a few test cases that cross validate the converter against the original PyTorch. We provide a utility function to do this, as described in the Custom Converter usage guide. Ideally pull requests solve one thing at a time. This makes it easy to evaluate the impact that the changes have on the project step-by-step. The more confident we are that the changes will not adversely impact the experience of other developers, the more likely we are to accept them. Running module test cases Before any change is accepted, we run the test cases on at least one platform. This performs a large number of cross validation checks against PyTorch. To do this python3 -m torch2trt.test --name = converters --tolerance = 1e-2 This will not hard-fail, but will highlight any build errors or max error checks. It is helpful if you include the status of this command in any pull-request, as well as system information like PyTorch version TensorRT version Platform (ie: Jetson Nano) Testing documentation If you have a change that modifies the documentation, it is relatively straightforward to test. We use mkdocs-material for documentation, which parses markdown files in the docs folder. To view the docs, simply call ./scripts/test_docs.sh And then navigate to https://<ip_address>:8000 . Please note, this will not include dynamically generated documentation pages like the converters page. These contain cross reference links to the GitHub source code. If you want to test these you can call ./scripts/build_docs.sh <github url> <tag> Pointing to the public reflection of your local repository. For example, if we're working off the upstream master branch, we would call ./scripts/build_docs.sh https://github.com/NVIDIA-AI-IOT/torch2trt master If your changes are pushed to your fork, you would do ./scripts/build_docs.sh https://github.com/<user>/torch2trt my_branch","title":"Contributing"},{"location":"CONTRIBUTING.html#contributing","text":"","title":"Contributing"},{"location":"CONTRIBUTING.html#forms-of-contribution","text":"","title":"Forms of contribution"},{"location":"CONTRIBUTING.html#submit-an-issue","text":"torch2trt is use case driven. We originally created it to solve use cases related to NVIDIA Jetson, but the layer support has grown largely since it's release and we've found that it has helped many other developers as well. The growth of torch2trt has been largely driven by issues submitted on GitHub . We learn a lot from the reported issues. Submitting an issue it is one of the best ways to begin contributing to torch2trt. The reported issues typically are one of the following, A bug or unexpected result A model with unsupported layers If you report an issue, we typically find the following information helpful PyTorch version TensorRT version Platform (ie: Jetson Nano) The PyTorch Module you're attempting to convert The steps taken to convert the PyTorch module If you're not sure how to provide any of these pieces of information, don't worry. Just open the issue and we're happy to discuss and help work out the details.","title":"Submit an Issue"},{"location":"CONTRIBUTING.html#ask-a-question","text":"Another great way to contribute is to ask a question on GitHub . There are often other developers who share your question, and they may find the discussion helpful. This also helps us gauge feature interest and identify gaps in documentation.","title":"Ask a Question"},{"location":"CONTRIBUTING.html#submit-a-pull-request","text":"torch2trt is use case driven and has limited maintainence, for this reason we value community contributions greatly. Another great way to contribute is by submitting a pull request. Pull requests which are most likely to be accepted are A new converter A test case A bug fix If you add a new converter, it is best to include a few test cases that cross validate the converter against the original PyTorch. We provide a utility function to do this, as described in the Custom Converter usage guide. Ideally pull requests solve one thing at a time. This makes it easy to evaluate the impact that the changes have on the project step-by-step. The more confident we are that the changes will not adversely impact the experience of other developers, the more likely we are to accept them.","title":"Submit a Pull Request"},{"location":"CONTRIBUTING.html#running-module-test-cases","text":"Before any change is accepted, we run the test cases on at least one platform. This performs a large number of cross validation checks against PyTorch. To do this python3 -m torch2trt.test --name = converters --tolerance = 1e-2 This will not hard-fail, but will highlight any build errors or max error checks. It is helpful if you include the status of this command in any pull-request, as well as system information like PyTorch version TensorRT version Platform (ie: Jetson Nano)","title":"Running module test cases"},{"location":"CONTRIBUTING.html#testing-documentation","text":"If you have a change that modifies the documentation, it is relatively straightforward to test. We use mkdocs-material for documentation, which parses markdown files in the docs folder. To view the docs, simply call ./scripts/test_docs.sh And then navigate to https://<ip_address>:8000 . Please note, this will not include dynamically generated documentation pages like the converters page. These contain cross reference links to the GitHub source code. If you want to test these you can call ./scripts/build_docs.sh <github url> <tag> Pointing to the public reflection of your local repository. For example, if we're working off the upstream master branch, we would call ./scripts/build_docs.sh https://github.com/NVIDIA-AI-IOT/torch2trt master If your changes are pushed to your fork, you would do ./scripts/build_docs.sh https://github.com/<user>/torch2trt my_branch","title":"Testing documentation"},{"location":"converters.html","text":"Converters This table contains a list of supported PyTorch methods and their associated converters. If your model is not converting, a good start in debugging would be to see if it contains a method not listed in this table. You may also find these a useful reference when writing your own converters. Method Converter torch.abs convert_abs torch.abs_ convert_abs torch.acos convert_acos torch.acos_ convert_acos torch.add convert_add torch.asin convert_asin torch.asin_ convert_asin torch.atan convert_atan torch.atan_ convert_atan torch.cat convert_cat torch.ceil convert_ceil torch.ceil_ convert_ceil torch.chunk convert_chunk torch.clamp convert_clamp torch.clamp_max convert_clamp_max torch.clamp_min convert_clamp_min torch.cos convert_cos torch.cos_ convert_cos torch.cosh convert_cosh torch.cosh_ convert_cosh torch.div convert_div torch.exp convert_exp torch.exp_ convert_exp torch.flatten convert_view torch.floor convert_floor torch.floor_ convert_floor torch.instance_norm convert_instance_norm torch.log convert_log torch.log_ convert_log torch.max convert_max torch.mean convert_mean torch.min convert_min torch.mul convert_mul torch.narrow convert_narrow torch.neg convert_neg torch.neg_ convert_neg torch.pow convert_pow torch.prod convert_prod torch.reciprocal convert_reciprocal torch.reciprocal_ convert_reciprocal torch.relu convert_relu torch.relu_ convert_relu torch.selu convert_selu torch.selu_ convert_selu torch.sigmoid convert_sigmoid torch.sin convert_sin torch.sin_ convert_sin torch.sinh convert_sinh torch.sinh_ convert_sinh torch.split convert_split torch.sqrt convert_sqrt torch.sqrt_ convert_sqrt torch.squeeze convert_view torch.sub convert_sub torch.sum convert_sum torch.tan convert_cos torch.tan_ convert_cos torch.tanh convert_tanh torch.transpose convert_transpose torch.unsqueeze convert_view torch.Tensor.__add__ convert_add torch.Tensor.__div__ convert_div torch.Tensor.__iadd__ convert_add torch.Tensor.__idiv__ convert_div torch.Tensor.__imul__ convert_mul torch.Tensor.__ipow__ convert_pow torch.Tensor.__isub__ convert_sub torch.Tensor.__itruediv__ convert_div torch.Tensor.__mul__ convert_mul torch.Tensor.__neg__ convert_neg torch.Tensor.__pow__ convert_pow torch.Tensor.__radd__ convert_add torch.Tensor.__rdiv__ convert_rdiv torch.Tensor.__rmul__ convert_mul torch.Tensor.__rpow__ convert_pow torch.Tensor.__rsub__ convert_sub torch.Tensor.__rtruediv__ convert_rdiv torch.Tensor.__sub__ convert_sub torch.Tensor.__truediv__ convert_div torch.Tensor.abs convert_abs torch.Tensor.abs_ convert_abs torch.Tensor.acos convert_acos torch.Tensor.acos_ convert_acos torch.Tensor.asin convert_asin torch.Tensor.asin_ convert_asin torch.Tensor.atan convert_atan torch.Tensor.atan_ convert_atan torch.Tensor.ceil convert_ceil torch.Tensor.ceil_ convert_ceil torch.Tensor.chunk convert_chunk torch.Tensor.clamp convert_clamp torch.Tensor.clamp_max convert_clamp_max torch.Tensor.clamp_min convert_clamp_min torch.Tensor.contiguous convert_identity torch.Tensor.cos convert_cos torch.Tensor.cos_ convert_cos torch.Tensor.cosh convert_cosh torch.Tensor.cosh_ convert_cosh torch.Tensor.exp convert_exp torch.Tensor.exp_ convert_exp torch.Tensor.floor convert_floor torch.Tensor.floor_ convert_floor torch.Tensor.log convert_log torch.Tensor.log_ convert_log torch.Tensor.max convert_max torch.Tensor.mean convert_mean torch.Tensor.min convert_min torch.Tensor.narrow convert_narrow torch.Tensor.neg convert_neg torch.Tensor.neg_ convert_neg torch.Tensor.permute convert_permute torch.Tensor.prod convert_prod torch.Tensor.reciprocal convert_reciprocal torch.Tensor.reciprocal_ convert_reciprocal torch.Tensor.reshape convert_view torch.Tensor.sin convert_sin torch.Tensor.sin_ convert_sin torch.Tensor.sinh convert_sinh torch.Tensor.sinh_ convert_sinh torch.Tensor.split convert_split torch.Tensor.sqrt convert_sqrt torch.Tensor.sqrt_ convert_sqrt torch.Tensor.squeeze convert_view torch.Tensor.sum convert_sum torch.Tensor.tan convert_cos torch.Tensor.tan_ convert_cos torch.Tensor.unsqueeze convert_view torch.Tensor.view convert_view torch.nn.functional.adaptive_avg_pool2d convert_adaptive_avg_pool2d torch.nn.functional.adaptive_max_pool2d convert_adaptive_max_pool2d torch.nn.functional.avg_pool2d convert_avg_pool2d torch.nn.functional.dropout convert_identity torch.nn.functional.dropout2d convert_identity torch.nn.functional.dropout3d convert_identity torch.nn.functional.elu convert_elu torch.nn.functional.elu_ convert_elu torch.nn.functional.instance_norm convert_instance_norm torch.nn.functional.leaky_relu convert_leaky_relu torch.nn.functional.leaky_relu_ convert_leaky_relu torch.nn.functional.max_pool2d convert_max_pool2d torch.nn.functional.normalize convert_normalize torch.nn.functional.pad convert_pad torch.nn.functional.prelu convert_prelu torch.nn.functional.relu convert_relu torch.nn.functional.relu6 convert_relu6 torch.nn.functional.relu_ convert_relu torch.nn.functional.selu convert_selu torch.nn.functional.selu_ convert_selu torch.nn.functional.sigmoid convert_sigmoid torch.nn.functional.softmax convert_softmax torch.nn.functional.softplus convert_softplus torch.nn.functional.softsign convert_softsign torch.nn.functional.tanh convert_tanh torch.nn.AdaptiveAvgPool2d.forward convert_AdaptiveAvgPool2d torch.nn.BatchNorm1d.forward convert_BatchNorm2d torch.nn.BatchNorm2d.forward convert_BatchNorm2d torch.nn.Conv1d.forward convert_Conv1d torch.nn.Conv2d.forward convert_Conv2d torch.nn.ConvTranspose2d.forward convert_ConvTranspose2d torch.nn.Dropout3d.forward convert_Identity torch.nn.Dropout2d.forward convert_Identity torch.nn.Dropout.forward convert_Identity torch.nn.Linear.forward convert_Linear torch.nn.LogSoftmax.forward convert_LogSoftmax torch.nn.ReLU.forward convert_ReLU torch.nn.ReLU6.forward convert_ReLU6 torch.Tensor.__getitem__ convert_tensor_getitem","title":"Converters"},{"location":"converters.html#converters","text":"This table contains a list of supported PyTorch methods and their associated converters. If your model is not converting, a good start in debugging would be to see if it contains a method not listed in this table. You may also find these a useful reference when writing your own converters. Method Converter torch.abs convert_abs torch.abs_ convert_abs torch.acos convert_acos torch.acos_ convert_acos torch.add convert_add torch.asin convert_asin torch.asin_ convert_asin torch.atan convert_atan torch.atan_ convert_atan torch.cat convert_cat torch.ceil convert_ceil torch.ceil_ convert_ceil torch.chunk convert_chunk torch.clamp convert_clamp torch.clamp_max convert_clamp_max torch.clamp_min convert_clamp_min torch.cos convert_cos torch.cos_ convert_cos torch.cosh convert_cosh torch.cosh_ convert_cosh torch.div convert_div torch.exp convert_exp torch.exp_ convert_exp torch.flatten convert_view torch.floor convert_floor torch.floor_ convert_floor torch.instance_norm convert_instance_norm torch.log convert_log torch.log_ convert_log torch.max convert_max torch.mean convert_mean torch.min convert_min torch.mul convert_mul torch.narrow convert_narrow torch.neg convert_neg torch.neg_ convert_neg torch.pow convert_pow torch.prod convert_prod torch.reciprocal convert_reciprocal torch.reciprocal_ convert_reciprocal torch.relu convert_relu torch.relu_ convert_relu torch.selu convert_selu torch.selu_ convert_selu torch.sigmoid convert_sigmoid torch.sin convert_sin torch.sin_ convert_sin torch.sinh convert_sinh torch.sinh_ convert_sinh torch.split convert_split torch.sqrt convert_sqrt torch.sqrt_ convert_sqrt torch.squeeze convert_view torch.sub convert_sub torch.sum convert_sum torch.tan convert_cos torch.tan_ convert_cos torch.tanh convert_tanh torch.transpose convert_transpose torch.unsqueeze convert_view torch.Tensor.__add__ convert_add torch.Tensor.__div__ convert_div torch.Tensor.__iadd__ convert_add torch.Tensor.__idiv__ convert_div torch.Tensor.__imul__ convert_mul torch.Tensor.__ipow__ convert_pow torch.Tensor.__isub__ convert_sub torch.Tensor.__itruediv__ convert_div torch.Tensor.__mul__ convert_mul torch.Tensor.__neg__ convert_neg torch.Tensor.__pow__ convert_pow torch.Tensor.__radd__ convert_add torch.Tensor.__rdiv__ convert_rdiv torch.Tensor.__rmul__ convert_mul torch.Tensor.__rpow__ convert_pow torch.Tensor.__rsub__ convert_sub torch.Tensor.__rtruediv__ convert_rdiv torch.Tensor.__sub__ convert_sub torch.Tensor.__truediv__ convert_div torch.Tensor.abs convert_abs torch.Tensor.abs_ convert_abs torch.Tensor.acos convert_acos torch.Tensor.acos_ convert_acos torch.Tensor.asin convert_asin torch.Tensor.asin_ convert_asin torch.Tensor.atan convert_atan torch.Tensor.atan_ convert_atan torch.Tensor.ceil convert_ceil torch.Tensor.ceil_ convert_ceil torch.Tensor.chunk convert_chunk torch.Tensor.clamp convert_clamp torch.Tensor.clamp_max convert_clamp_max torch.Tensor.clamp_min convert_clamp_min torch.Tensor.contiguous convert_identity torch.Tensor.cos convert_cos torch.Tensor.cos_ convert_cos torch.Tensor.cosh convert_cosh torch.Tensor.cosh_ convert_cosh torch.Tensor.exp convert_exp torch.Tensor.exp_ convert_exp torch.Tensor.floor convert_floor torch.Tensor.floor_ convert_floor torch.Tensor.log convert_log torch.Tensor.log_ convert_log torch.Tensor.max convert_max torch.Tensor.mean convert_mean torch.Tensor.min convert_min torch.Tensor.narrow convert_narrow torch.Tensor.neg convert_neg torch.Tensor.neg_ convert_neg torch.Tensor.permute convert_permute torch.Tensor.prod convert_prod torch.Tensor.reciprocal convert_reciprocal torch.Tensor.reciprocal_ convert_reciprocal torch.Tensor.reshape convert_view torch.Tensor.sin convert_sin torch.Tensor.sin_ convert_sin torch.Tensor.sinh convert_sinh torch.Tensor.sinh_ convert_sinh torch.Tensor.split convert_split torch.Tensor.sqrt convert_sqrt torch.Tensor.sqrt_ convert_sqrt torch.Tensor.squeeze convert_view torch.Tensor.sum convert_sum torch.Tensor.tan convert_cos torch.Tensor.tan_ convert_cos torch.Tensor.unsqueeze convert_view torch.Tensor.view convert_view torch.nn.functional.adaptive_avg_pool2d convert_adaptive_avg_pool2d torch.nn.functional.adaptive_max_pool2d convert_adaptive_max_pool2d torch.nn.functional.avg_pool2d convert_avg_pool2d torch.nn.functional.dropout convert_identity torch.nn.functional.dropout2d convert_identity torch.nn.functional.dropout3d convert_identity torch.nn.functional.elu convert_elu torch.nn.functional.elu_ convert_elu torch.nn.functional.instance_norm convert_instance_norm torch.nn.functional.leaky_relu convert_leaky_relu torch.nn.functional.leaky_relu_ convert_leaky_relu torch.nn.functional.max_pool2d convert_max_pool2d torch.nn.functional.normalize convert_normalize torch.nn.functional.pad convert_pad torch.nn.functional.prelu convert_prelu torch.nn.functional.relu convert_relu torch.nn.functional.relu6 convert_relu6 torch.nn.functional.relu_ convert_relu torch.nn.functional.selu convert_selu torch.nn.functional.selu_ convert_selu torch.nn.functional.sigmoid convert_sigmoid torch.nn.functional.softmax convert_softmax torch.nn.functional.softplus convert_softplus torch.nn.functional.softsign convert_softsign torch.nn.functional.tanh convert_tanh torch.nn.AdaptiveAvgPool2d.forward convert_AdaptiveAvgPool2d torch.nn.BatchNorm1d.forward convert_BatchNorm2d torch.nn.BatchNorm2d.forward convert_BatchNorm2d torch.nn.Conv1d.forward convert_Conv1d torch.nn.Conv2d.forward convert_Conv2d torch.nn.ConvTranspose2d.forward convert_ConvTranspose2d torch.nn.Dropout3d.forward convert_Identity torch.nn.Dropout2d.forward convert_Identity torch.nn.Dropout.forward convert_Identity torch.nn.Linear.forward convert_Linear torch.nn.LogSoftmax.forward convert_LogSoftmax torch.nn.ReLU.forward convert_ReLU torch.nn.ReLU6.forward convert_ReLU6 torch.Tensor.__getitem__ convert_tensor_getitem","title":"Converters"},{"location":"getting_started.html","text":"Getting Started Follow these steps to get started using torch2trt. Note torch2trt depends on the TensorRT Python API. On Jetson, this is included with the latest JetPack. For desktop, please follow the TensorRT Installation Guide . You may also try installing torch2trt inside one of the NGC PyTorch docker containers for Desktop or Jetson . Install Without plugins To install without compiling plugins, call the following git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt python setup.py install Install With plugins To install with plugins to support some operations in PyTorch that are not natviely supported with TensorRT, call the following Note Please note, this currently only includes the interpolate plugin. This plugin requires PyTorch 1.3+ for serialization. git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt sudo python setup.py install --plugins","title":"Getting Started"},{"location":"getting_started.html#getting-started","text":"Follow these steps to get started using torch2trt. Note torch2trt depends on the TensorRT Python API. On Jetson, this is included with the latest JetPack. For desktop, please follow the TensorRT Installation Guide . You may also try installing torch2trt inside one of the NGC PyTorch docker containers for Desktop or Jetson .","title":"Getting Started"},{"location":"getting_started.html#install-without-plugins","text":"To install without compiling plugins, call the following git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt python setup.py install","title":"Install Without plugins"},{"location":"getting_started.html#install-with-plugins","text":"To install with plugins to support some operations in PyTorch that are not natviely supported with TensorRT, call the following Note Please note, this currently only includes the interpolate plugin. This plugin requires PyTorch 1.3+ for serialization. git clone https://github.com/NVIDIA-AI-IOT/torch2trt cd torch2trt sudo python setup.py install --plugins","title":"Install With plugins"},{"location":"see_also.html","text":"See Also Note The state of these converters may change over time. We provide this information here with the hope that it will help shed light on the landscape of tools available for optimizing PyTorch models with TensorRT. If you find this information helpful or outdated / misleading, please let us know. In addition to torch2trt, there are other workflows for optimizing your PyTorch model with TensorRT. The other converters we are aware of are ONNX to TensorRT Tip Since the ONNX parser ships with TensorRT, we have included a convenience method for using this workflow with torch2trt. If you want to quickly try the ONNX method using the torch2trt interface, just call torch2trt(..., use_onnx=True) . This will perform conversion on the module by exporting the model using PyTorch's JIT tracer, and parsing with TensorRT's ONNX parser. TRTorch Which one you use depends largely on your use case. The differences often come down to Layer support Modern deep learning frameworks are large, and there often arise caveats converting between frameworks using a given workflow. These could include limitations in serialization or parsing formats. Or in some instances, it may be possible the layer could be supported, but it has just not been done yet. TRTorch is strong in the sense that it will default to the original PyTorch method for layers which are not converted to TensorRT. The best way to know which conversion method works for you is to try converting your model. Feature support TensorRT is evolving and the conversion workflows may have varying level of feature support. In some instances, you may wish to use a latest feature of TensorRT, like dynamic shapes, but it is not supported in torch2trt or the interface has not yet been exposed. In this instance, we recommend checking to see if it is supported by one of the other workflows. The ONNX converter is typically strong in this regards, since the parser is distributed with TensorRT. Note If there is a TensorRT feature you wished to see in torch2trt, please let us know. We can not gaurantee this will be done, but it helps us gauge interest. Extensibility / Ease of Use In case none of the converters satisfy for your use case, you may find it necessary to adapt the converter to fit your needs. This is very intuitive with torch2trt, since it is done inline with Python, and there are many examples to reference. If you know how the original PyTorch method works, and have the TensorRT Python API on hand, it is relatively straight forward to adapt torch2trt to your needs. The extensibility is often helpful when you want to implement a converter that is specific to the context the layer appears in.","title":"See Also"},{"location":"see_also.html#see-also","text":"Note The state of these converters may change over time. We provide this information here with the hope that it will help shed light on the landscape of tools available for optimizing PyTorch models with TensorRT. If you find this information helpful or outdated / misleading, please let us know. In addition to torch2trt, there are other workflows for optimizing your PyTorch model with TensorRT. The other converters we are aware of are ONNX to TensorRT Tip Since the ONNX parser ships with TensorRT, we have included a convenience method for using this workflow with torch2trt. If you want to quickly try the ONNX method using the torch2trt interface, just call torch2trt(..., use_onnx=True) . This will perform conversion on the module by exporting the model using PyTorch's JIT tracer, and parsing with TensorRT's ONNX parser. TRTorch Which one you use depends largely on your use case. The differences often come down to","title":"See Also"},{"location":"see_also.html#layer-support","text":"Modern deep learning frameworks are large, and there often arise caveats converting between frameworks using a given workflow. These could include limitations in serialization or parsing formats. Or in some instances, it may be possible the layer could be supported, but it has just not been done yet. TRTorch is strong in the sense that it will default to the original PyTorch method for layers which are not converted to TensorRT. The best way to know which conversion method works for you is to try converting your model.","title":"Layer support"},{"location":"see_also.html#feature-support","text":"TensorRT is evolving and the conversion workflows may have varying level of feature support. In some instances, you may wish to use a latest feature of TensorRT, like dynamic shapes, but it is not supported in torch2trt or the interface has not yet been exposed. In this instance, we recommend checking to see if it is supported by one of the other workflows. The ONNX converter is typically strong in this regards, since the parser is distributed with TensorRT. Note If there is a TensorRT feature you wished to see in torch2trt, please let us know. We can not gaurantee this will be done, but it helps us gauge interest.","title":"Feature support"},{"location":"see_also.html#extensibility-ease-of-use","text":"In case none of the converters satisfy for your use case, you may find it necessary to adapt the converter to fit your needs. This is very intuitive with torch2trt, since it is done inline with Python, and there are many examples to reference. If you know how the original PyTorch method works, and have the TensorRT Python API on hand, it is relatively straight forward to adapt torch2trt to your needs. The extensibility is often helpful when you want to implement a converter that is specific to the context the layer appears in.","title":"Extensibility / Ease of Use"},{"location":"benchmarks/jetson_nano.html","text":"Jetson Nano Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torchvision.models.alexnet.alexnet float16 [(1, 3, 224, 224)] 2.29E-05 46.4 69.9 22.1 14.7 torchvision.models.squeezenet.squeezenet1_0 float16 [(1, 3, 224, 224)] 1.20E-02 44 137 24.2 7.6 torchvision.models.squeezenet.squeezenet1_1 float16 [(1, 3, 224, 224)] 9.77E-04 76.6 248 14 4.34 torchvision.models.resnet.resnet18 float16 [(1, 3, 224, 224)] 5.86E-03 29.4 90.2 34.7 11.4 torchvision.models.resnet.resnet34 float16 [(1, 3, 224, 224)] 1.56E-01 15.5 50.7 64.8 20.2 torchvision.models.resnet.resnet50 float16 [(1, 3, 224, 224)] 6.45E-02 12.4 34.2 81.7 29.8 torchvision.models.resnet.resnet101 float16 [(1, 3, 224, 224)] 1.01E+03 7.18 19.9 141 51.1 torchvision.models.resnet.resnet152 float16 [(1, 3, 224, 224)] 0.00E+00 4.96 14.1 204 72.3 torchvision.models.densenet.densenet121 float16 [(1, 3, 224, 224)] 3.42E-03 11.5 41.9 84.5 24.8 torchvision.models.densenet.densenet169 float16 [(1, 3, 224, 224)] 5.86E-03 8.25 33.2 118 31.2 torchvision.models.densenet.densenet201 float16 [(1, 3, 224, 224)] 3.42E-03 6.84 25.4 141 40.8 torchvision.models.densenet.densenet161 float16 [(1, 3, 224, 224)] 4.15E-03 4.71 15.6 247 65.8 torchvision.models.vgg.vgg11 float16 [(1, 3, 224, 224)] 3.51E-04 8.9 18.3 114 55.1 torchvision.models.vgg.vgg13 float16 [(1, 3, 224, 224)] 3.07E-04 6.53 14.7 156 68.7 torchvision.models.vgg.vgg16 float16 [(1, 3, 224, 224)] 4.58E-04 5.09 11.9 201 85.1 torchvision.models.vgg.vgg11_bn float16 [(1, 3, 224, 224)] 3.81E-04 8.74 18.4 117 54.8 torchvision.models.vgg.vgg13_bn float16 [(1, 3, 224, 224)] 5.19E-04 6.31 14.8 162 68.5 torchvision.models.vgg.vgg16_bn float16 [(1, 3, 224, 224)] 9.77E-04 4.96 12 207 84.3","title":"Jetson Nano"},{"location":"benchmarks/jetson_nano.html#jetson-nano","text":"Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torchvision.models.alexnet.alexnet float16 [(1, 3, 224, 224)] 2.29E-05 46.4 69.9 22.1 14.7 torchvision.models.squeezenet.squeezenet1_0 float16 [(1, 3, 224, 224)] 1.20E-02 44 137 24.2 7.6 torchvision.models.squeezenet.squeezenet1_1 float16 [(1, 3, 224, 224)] 9.77E-04 76.6 248 14 4.34 torchvision.models.resnet.resnet18 float16 [(1, 3, 224, 224)] 5.86E-03 29.4 90.2 34.7 11.4 torchvision.models.resnet.resnet34 float16 [(1, 3, 224, 224)] 1.56E-01 15.5 50.7 64.8 20.2 torchvision.models.resnet.resnet50 float16 [(1, 3, 224, 224)] 6.45E-02 12.4 34.2 81.7 29.8 torchvision.models.resnet.resnet101 float16 [(1, 3, 224, 224)] 1.01E+03 7.18 19.9 141 51.1 torchvision.models.resnet.resnet152 float16 [(1, 3, 224, 224)] 0.00E+00 4.96 14.1 204 72.3 torchvision.models.densenet.densenet121 float16 [(1, 3, 224, 224)] 3.42E-03 11.5 41.9 84.5 24.8 torchvision.models.densenet.densenet169 float16 [(1, 3, 224, 224)] 5.86E-03 8.25 33.2 118 31.2 torchvision.models.densenet.densenet201 float16 [(1, 3, 224, 224)] 3.42E-03 6.84 25.4 141 40.8 torchvision.models.densenet.densenet161 float16 [(1, 3, 224, 224)] 4.15E-03 4.71 15.6 247 65.8 torchvision.models.vgg.vgg11 float16 [(1, 3, 224, 224)] 3.51E-04 8.9 18.3 114 55.1 torchvision.models.vgg.vgg13 float16 [(1, 3, 224, 224)] 3.07E-04 6.53 14.7 156 68.7 torchvision.models.vgg.vgg16 float16 [(1, 3, 224, 224)] 4.58E-04 5.09 11.9 201 85.1 torchvision.models.vgg.vgg11_bn float16 [(1, 3, 224, 224)] 3.81E-04 8.74 18.4 117 54.8 torchvision.models.vgg.vgg13_bn float16 [(1, 3, 224, 224)] 5.19E-04 6.31 14.8 162 68.5 torchvision.models.vgg.vgg16_bn float16 [(1, 3, 224, 224)] 9.77E-04 4.96 12 207 84.3","title":"Jetson Nano"},{"location":"benchmarks/jetson_xavier.html","text":"Jetson Xavier Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torch2trt.tests.torchvision.classification.alexnet float16 [(1, 3, 224, 224)] 7.63E-05 251 565 4.96 2.02 torch2trt.tests.torchvision.classification.squeezenet1_0 float16 [(1, 3, 224, 224)] 9.77E-04 121 834 8.04 1.49 torch2trt.tests.torchvision.classification.squeezenet1_1 float16 [(1, 3, 224, 224)] 9.77E-04 125 1.29e+03 8.01 1.02 torch2trt.tests.torchvision.classification.resnet18 float16 [(1, 3, 224, 224)] 9.77E-03 136 722 7.33 1.64 torch2trt.tests.torchvision.classification.resnet34 float16 [(1, 3, 224, 224)] 2.50E-01 77.8 396 12.9 2.79 torch2trt.tests.torchvision.classification.resnet50 float16 [(1, 3, 224, 224)] 1.09E-01 55.8 326 17.9 3.37 torch2trt.tests.torchvision.classification.resnet101 float16 [(1, 3, 224, 224)] 0.00E+00 28.3 175 35.1 6.04 torch2trt.tests.torchvision.classification.resnet152 float16 [(1, 3, 224, 224)] 0.00E+00 18.8 122 53.2 8.57 torch2trt.tests.torchvision.classification.densenet121 float16 [(1, 3, 224, 224)] 7.81E-03 20.9 76.6 47.5 13 torch2trt.tests.torchvision.classification.densenet169 float16 [(1, 3, 224, 224)] 3.91E-03 14.8 41.7 66.7 23.7 torch2trt.tests.torchvision.classification.densenet201 float16 [(1, 3, 224, 224)] 4.88E-03 12.6 30.2 79.1 33 torch2trt.tests.torchvision.classification.densenet161 float16 [(1, 3, 224, 224)] 4.88E-03 16.1 43.7 62.1 23 torch2trt.tests.torchvision.classification.vgg11 float16 [(1, 3, 224, 224)] 2.56E-03 84.8 201 12.1 5.24 torch2trt.tests.torchvision.classification.vgg13 float16 [(1, 3, 224, 224)] 2.24E-03 71.1 165 14.3 6.34 torch2trt.tests.torchvision.classification.vgg16 float16 [(1, 3, 224, 224)] 3.78E-03 61.5 139 16.5 7.46 torch2trt.tests.torchvision.classification.vgg19 float16 [(1, 3, 224, 224)] 2.81E-03 54.1 120 18.7 8.61 torch2trt.tests.torchvision.classification.vgg11_bn float16 [(1, 3, 224, 224)] 2.20E-03 81.5 200 12.5 5.27 torch2trt.tests.torchvision.classification.vgg13_bn float16 [(1, 3, 224, 224)] 1.71E-03 67.5 165 15.1 6.33 torch2trt.tests.torchvision.classification.vgg16_bn float16 [(1, 3, 224, 224)] 2.87E-03 58.3 139 17.4 7.48 torch2trt.tests.torchvision.classification.vgg19_bn float16 [(1, 3, 224, 224)] 2.44E-03 51.4 120 19.7 8.61 torch2trt.tests.torchvision.classification.mobilenet_v2 float16 [(1, 3, 224, 224)] 0.00E+00 64.8 723 15.4 1.67 torch2trt.tests.torchvision.classification.shufflenet_v2_x0_5 float16 [(1, 3, 224, 224)] 1.53E-05 51.2 463 19.4 2.17 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_0 float16 [(1, 3, 224, 224)] 1.53E-05 49.4 419 20.4 2.43 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_5 float16 [(1, 3, 224, 224)] 1.53E-05 51.4 426 19.6 2.37 torch2trt.tests.torchvision.classification.shufflenet_v2_x2_0 float16 [(1, 3, 224, 224)] 1.53E-05 48.2 419 20.8 2.48 torch2trt.tests.torchvision.classification.mnasnet0_5 float16 [(1, 3, 224, 224)] 2.03E-06 67.8 883 14.9 1.4 torch2trt.tests.torchvision.classification.mnasnet0_75 float16 [(1, 3, 224, 224)] 0.00E+00 67.6 751 14.8 1.6 torch2trt.tests.torchvision.classification.mnasnet1_0 float16 [(1, 3, 224, 224)] 0.00E+00 65.7 667 15.2 1.77 torch2trt.tests.torchvision.classification.mnasnet1_3 float16 [(1, 3, 224, 224)] 0.00E+00 67.4 573 15 2.02","title":"Jetson Xavier"},{"location":"benchmarks/jetson_xavier.html#jetson-xavier","text":"Name Data Type Input Shapes torch2trt kwargs Max Error Throughput (PyTorch) Throughput (TensorRT) Latency (PyTorch) Latency (TensorRT) torch2trt.tests.torchvision.classification.alexnet float16 [(1, 3, 224, 224)] 7.63E-05 251 565 4.96 2.02 torch2trt.tests.torchvision.classification.squeezenet1_0 float16 [(1, 3, 224, 224)] 9.77E-04 121 834 8.04 1.49 torch2trt.tests.torchvision.classification.squeezenet1_1 float16 [(1, 3, 224, 224)] 9.77E-04 125 1.29e+03 8.01 1.02 torch2trt.tests.torchvision.classification.resnet18 float16 [(1, 3, 224, 224)] 9.77E-03 136 722 7.33 1.64 torch2trt.tests.torchvision.classification.resnet34 float16 [(1, 3, 224, 224)] 2.50E-01 77.8 396 12.9 2.79 torch2trt.tests.torchvision.classification.resnet50 float16 [(1, 3, 224, 224)] 1.09E-01 55.8 326 17.9 3.37 torch2trt.tests.torchvision.classification.resnet101 float16 [(1, 3, 224, 224)] 0.00E+00 28.3 175 35.1 6.04 torch2trt.tests.torchvision.classification.resnet152 float16 [(1, 3, 224, 224)] 0.00E+00 18.8 122 53.2 8.57 torch2trt.tests.torchvision.classification.densenet121 float16 [(1, 3, 224, 224)] 7.81E-03 20.9 76.6 47.5 13 torch2trt.tests.torchvision.classification.densenet169 float16 [(1, 3, 224, 224)] 3.91E-03 14.8 41.7 66.7 23.7 torch2trt.tests.torchvision.classification.densenet201 float16 [(1, 3, 224, 224)] 4.88E-03 12.6 30.2 79.1 33 torch2trt.tests.torchvision.classification.densenet161 float16 [(1, 3, 224, 224)] 4.88E-03 16.1 43.7 62.1 23 torch2trt.tests.torchvision.classification.vgg11 float16 [(1, 3, 224, 224)] 2.56E-03 84.8 201 12.1 5.24 torch2trt.tests.torchvision.classification.vgg13 float16 [(1, 3, 224, 224)] 2.24E-03 71.1 165 14.3 6.34 torch2trt.tests.torchvision.classification.vgg16 float16 [(1, 3, 224, 224)] 3.78E-03 61.5 139 16.5 7.46 torch2trt.tests.torchvision.classification.vgg19 float16 [(1, 3, 224, 224)] 2.81E-03 54.1 120 18.7 8.61 torch2trt.tests.torchvision.classification.vgg11_bn float16 [(1, 3, 224, 224)] 2.20E-03 81.5 200 12.5 5.27 torch2trt.tests.torchvision.classification.vgg13_bn float16 [(1, 3, 224, 224)] 1.71E-03 67.5 165 15.1 6.33 torch2trt.tests.torchvision.classification.vgg16_bn float16 [(1, 3, 224, 224)] 2.87E-03 58.3 139 17.4 7.48 torch2trt.tests.torchvision.classification.vgg19_bn float16 [(1, 3, 224, 224)] 2.44E-03 51.4 120 19.7 8.61 torch2trt.tests.torchvision.classification.mobilenet_v2 float16 [(1, 3, 224, 224)] 0.00E+00 64.8 723 15.4 1.67 torch2trt.tests.torchvision.classification.shufflenet_v2_x0_5 float16 [(1, 3, 224, 224)] 1.53E-05 51.2 463 19.4 2.17 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_0 float16 [(1, 3, 224, 224)] 1.53E-05 49.4 419 20.4 2.43 torch2trt.tests.torchvision.classification.shufflenet_v2_x1_5 float16 [(1, 3, 224, 224)] 1.53E-05 51.4 426 19.6 2.37 torch2trt.tests.torchvision.classification.shufflenet_v2_x2_0 float16 [(1, 3, 224, 224)] 1.53E-05 48.2 419 20.8 2.48 torch2trt.tests.torchvision.classification.mnasnet0_5 float16 [(1, 3, 224, 224)] 2.03E-06 67.8 883 14.9 1.4 torch2trt.tests.torchvision.classification.mnasnet0_75 float16 [(1, 3, 224, 224)] 0.00E+00 67.6 751 14.8 1.6 torch2trt.tests.torchvision.classification.mnasnet1_0 float16 [(1, 3, 224, 224)] 0.00E+00 65.7 667 15.2 1.77 torch2trt.tests.torchvision.classification.mnasnet1_3 float16 [(1, 3, 224, 224)] 0.00E+00 67.4 573 15 2.02","title":"Jetson Xavier"},{"location":"usage/basic_usage.html","text":"Basic Usage This page demonstrates basic torch2trt usage. Conversion You can easily convert a PyTorch module by calling torch2trt passing example data as input, for example to convert alexnet we call import torch from torch2trt import torch2trt from torchvision.models.alexnet import alexnet # create some regular pytorch model... model = alexnet ( pretrained = True ) . eval () . cuda () # create example data x = torch . ones (( 1 , 3 , 224 , 224 )) . cuda () # convert to TensorRT feeding sample data as input model_trt = torch2trt ( model , [ x ]) Note Currently with torch2trt, once the model is converted, you must use the same input shapes during execution. The exception is the batch size, which can vary up to the value specified by the max_batch_size parameter. Executution We can execute the returned TRTModule just like the original PyTorch model. Here we execute the model and print the maximum absolute error. y = model ( x ) y_trt = model_trt ( x ) # check the output against PyTorch print ( torch . max ( torch . abs ( y - y_trt ))) Saving and loading We can save the model as a state_dict . torch . save ( model_trt . state_dict (), 'alexnet_trt.pth' ) We can load the saved model into a TRTModule from torch2trt import TRTModule model_trt = TRTModule () model_trt . load_state_dict ( torch . load ( 'alexnet_trt.pth' ))","title":"Basic Usage"},{"location":"usage/basic_usage.html#basic-usage","text":"This page demonstrates basic torch2trt usage.","title":"Basic Usage"},{"location":"usage/basic_usage.html#conversion","text":"You can easily convert a PyTorch module by calling torch2trt passing example data as input, for example to convert alexnet we call import torch from torch2trt import torch2trt from torchvision.models.alexnet import alexnet # create some regular pytorch model... model = alexnet ( pretrained = True ) . eval () . cuda () # create example data x = torch . ones (( 1 , 3 , 224 , 224 )) . cuda () # convert to TensorRT feeding sample data as input model_trt = torch2trt ( model , [ x ]) Note Currently with torch2trt, once the model is converted, you must use the same input shapes during execution. The exception is the batch size, which can vary up to the value specified by the max_batch_size parameter.","title":"Conversion"},{"location":"usage/basic_usage.html#executution","text":"We can execute the returned TRTModule just like the original PyTorch model. Here we execute the model and print the maximum absolute error. y = model ( x ) y_trt = model_trt ( x ) # check the output against PyTorch print ( torch . max ( torch . abs ( y - y_trt )))","title":"Executution"},{"location":"usage/basic_usage.html#saving-and-loading","text":"We can save the model as a state_dict . torch . save ( model_trt . state_dict (), 'alexnet_trt.pth' ) We can load the saved model into a TRTModule from torch2trt import TRTModule model_trt = TRTModule () model_trt . load_state_dict ( torch . load ( 'alexnet_trt.pth' ))","title":"Saving and loading"},{"location":"usage/custom_converter.html","text":"Custom Converter This page details how to extend or modify the behavior of torch2trt by implementing and registering custom converters. Background torch2trt works by attaching conversion functions (like convert_ReLU ) to the original PyTorch functional calls (like torch.nn.ReLU.forward ). The sample input data is passed through the network, just as before, except now whenever a registered function ( torch.nn.ReLU.forward ) is encountered, the corresponding converter ( convert_ReLU ) is also called afterwards. The converter is passed the arguments and return statement of the original PyTorch function, as well as the TensorRT network that is being constructed. The input tensors to the original PyTorch function are modified to have an attribute _trt , which is the TensorRT counterpart to the PyTorch tensor. The conversion function uses this _trt to add layers to the TensorRT network, and then sets the _trt attribute for relevant output tensors. Once the model is fully executed, the final tensors returns are marked as outputs of the TensorRT network, and the optimized TensorRT engine is built. Add a custom converter Here we show how to add a converter for the ReLU module using the TensorRT python API. import tensorrt as trt from torch2trt import tensorrt_converter @tensorrt_converter ( 'torch.nn.ReLU.forward' ) def convert_ReLU ( ctx ): input = ctx . method_args [ 1 ] output = ctx . method_return layer = ctx . network . add_activation ( input = input . _trt , type = trt . ActivationType . RELU ) output . _trt = layer . get_output ( 0 ) The converter takes one argument, a ConversionContext , which will contain the following ctx.network - The TensorRT network that is being constructed. ctx.method_args - Positional arguments that were passed to the specified PyTorch function. The _trt attribute is set for relevant input tensors. ctx.method_kwargs - Keyword arguments that were passed to the specified PyTorch function. ctx.method_return - The value returned by the specified PyTorch function. The converter must set the _trt attribute where relevant. Please see the converters page for a list of implemented converters and links to their source code. These may help in learning how to write converters.","title":"Custom Converter"},{"location":"usage/custom_converter.html#custom-converter","text":"This page details how to extend or modify the behavior of torch2trt by implementing and registering custom converters.","title":"Custom Converter"},{"location":"usage/custom_converter.html#background","text":"torch2trt works by attaching conversion functions (like convert_ReLU ) to the original PyTorch functional calls (like torch.nn.ReLU.forward ). The sample input data is passed through the network, just as before, except now whenever a registered function ( torch.nn.ReLU.forward ) is encountered, the corresponding converter ( convert_ReLU ) is also called afterwards. The converter is passed the arguments and return statement of the original PyTorch function, as well as the TensorRT network that is being constructed. The input tensors to the original PyTorch function are modified to have an attribute _trt , which is the TensorRT counterpart to the PyTorch tensor. The conversion function uses this _trt to add layers to the TensorRT network, and then sets the _trt attribute for relevant output tensors. Once the model is fully executed, the final tensors returns are marked as outputs of the TensorRT network, and the optimized TensorRT engine is built.","title":"Background"},{"location":"usage/custom_converter.html#add-a-custom-converter","text":"Here we show how to add a converter for the ReLU module using the TensorRT python API. import tensorrt as trt from torch2trt import tensorrt_converter @tensorrt_converter ( 'torch.nn.ReLU.forward' ) def convert_ReLU ( ctx ): input = ctx . method_args [ 1 ] output = ctx . method_return layer = ctx . network . add_activation ( input = input . _trt , type = trt . ActivationType . RELU ) output . _trt = layer . get_output ( 0 ) The converter takes one argument, a ConversionContext , which will contain the following ctx.network - The TensorRT network that is being constructed. ctx.method_args - Positional arguments that were passed to the specified PyTorch function. The _trt attribute is set for relevant input tensors. ctx.method_kwargs - Keyword arguments that were passed to the specified PyTorch function. ctx.method_return - The value returned by the specified PyTorch function. The converter must set the _trt attribute where relevant. Please see the converters page for a list of implemented converters and links to their source code. These may help in learning how to write converters.","title":"Add a custom converter"}]}